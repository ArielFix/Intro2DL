{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArielFix/Intro2DL/blob/OnGoingAssignment1/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 1. Music Century Classification\n",
        "\n",
        "**Assignment Responsible**: Natalie Lang.\n",
        "\n",
        "In this assignment, we will build models to predict which\n",
        "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
        "based on the Million Song Dataset. The data is available to download from the UCI \n",
        "Machine Learning Repository. Here are some links about the data:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Note that you are note allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## Question 1. Data (21%)\n",
        "\n",
        "Start by setting up a Google Colab notebook in which to do your work.\n",
        "Since you are working with a partner, you might find this link helpful:\n",
        "\n",
        "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "\n",
        "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together. \n",
        "\n",
        "To process and read the data, we use the popular `pandas` package for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
        "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
        "The first method is easier but slower, and the second method is a bit involved at first, but\n",
        "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
        "figuring how to do that now.\n",
        "\n",
        "Here are some resources to help you get started:\n",
        "\n",
        "- http.://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_",
        "outputId": "f628cbca-a7a8-4f86-971c-813f89a5498a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "load_from_drive = True\n",
        "\n",
        "if not load_from_drive:\n",
        "  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  csv_path = '/content/gdrive/My Drive/MSc/Courses/Into to Deep Learnig/Assignments/YearPredictionMSD.txt.zip' # TODO - UPDATE ME WITH THE TRUE PATH!\n",
        "\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path, names=t_label + x_labels)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bBEnj3zGq_",
        "outputId": "a3c14c11-872b-4a82-cb92-7b62146320da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        year      var1      var2      var3      var4      var5      var6  \\\n",
              "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "...      ...       ...       ...       ...       ...       ...       ...   \n",
              "515340  2006  51.28467  45.88068  22.19582  -5.53319  -3.61835 -16.36914   \n",
              "515341  2006  49.87870  37.93125  18.65987  -3.63581 -27.75665 -18.52988   \n",
              "515342  2006  45.12852  12.65758 -38.72018   8.80882 -29.29985  -2.28706   \n",
              "515343  2006  44.16614  32.38368  -3.34971  -2.49165 -19.59278 -18.67098   \n",
              "515344  2005  51.85726  59.11655  26.39436  -5.46030 -20.69012 -19.95528   \n",
              "\n",
              "            var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "...          ...       ...       ...  ...       ...        ...        ...   \n",
              "515340   2.12652   5.18160  -8.66890  ...   4.81440   -3.75991  -30.92584   \n",
              "515341   7.76108   3.56109  -2.50351  ...  32.38589  -32.75535  -61.05473   \n",
              "515342 -18.40424 -22.28726  -4.52429  ... -18.73598  -71.15954 -123.98443   \n",
              "515343   8.78428   4.02039 -12.01230  ...  67.16763  282.77624   -4.63677   \n",
              "515344  -6.72771   2.29590  10.31018  ... -11.50511  -69.18291   60.58456   \n",
              "\n",
              "            var84     var85     var86      var87     var88      var89  \\\n",
              "0        15.37344   1.11144 -23.08793   68.40795  -1.82223  -27.46348   \n",
              "1        42.87836  -9.90378 -32.22788   70.49388  12.04941   58.43453   \n",
              "2        10.93792  -0.07568  43.20130 -115.00698  -0.05859   39.67068   \n",
              "3       -46.67617 -12.51516  82.58061  -72.08993   9.90558  199.62971   \n",
              "4       -17.72522  -1.49237  -7.50035   51.76631   7.88713   55.66926   \n",
              "...           ...       ...       ...        ...       ...        ...   \n",
              "515340   26.33968  -5.03390  21.86037 -142.29410   3.42901  -41.14721   \n",
              "515341   56.65182  15.29965  95.88193  -10.63242  12.96552   92.11633   \n",
              "515342  121.26989  10.89629  34.62409 -248.61020  -6.07171   53.96319   \n",
              "515343  144.00125  21.62652 -29.72432   71.47198  20.32240   14.83107   \n",
              "515344   28.64599  -4.39620 -64.56491  -45.61012  -5.51512   32.35602   \n",
              "\n",
              "           var90  \n",
              "0        2.26327  \n",
              "1       26.92061  \n",
              "2       -0.66345  \n",
              "3       18.85382  \n",
              "4       28.74903  \n",
              "...          ...  \n",
              "515340 -15.46052  \n",
              "515341  10.88815  \n",
              "515342  -8.09364  \n",
              "515343  39.74909  \n",
              "515344  12.17352  \n",
              "\n",
              "[515345 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b54e164-72f2-497f-94e4-91bc3d9eea1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515340</th>\n",
              "      <td>2006</td>\n",
              "      <td>51.28467</td>\n",
              "      <td>45.88068</td>\n",
              "      <td>22.19582</td>\n",
              "      <td>-5.53319</td>\n",
              "      <td>-3.61835</td>\n",
              "      <td>-16.36914</td>\n",
              "      <td>2.12652</td>\n",
              "      <td>5.18160</td>\n",
              "      <td>-8.66890</td>\n",
              "      <td>...</td>\n",
              "      <td>4.81440</td>\n",
              "      <td>-3.75991</td>\n",
              "      <td>-30.92584</td>\n",
              "      <td>26.33968</td>\n",
              "      <td>-5.03390</td>\n",
              "      <td>21.86037</td>\n",
              "      <td>-142.29410</td>\n",
              "      <td>3.42901</td>\n",
              "      <td>-41.14721</td>\n",
              "      <td>-15.46052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515341</th>\n",
              "      <td>2006</td>\n",
              "      <td>49.87870</td>\n",
              "      <td>37.93125</td>\n",
              "      <td>18.65987</td>\n",
              "      <td>-3.63581</td>\n",
              "      <td>-27.75665</td>\n",
              "      <td>-18.52988</td>\n",
              "      <td>7.76108</td>\n",
              "      <td>3.56109</td>\n",
              "      <td>-2.50351</td>\n",
              "      <td>...</td>\n",
              "      <td>32.38589</td>\n",
              "      <td>-32.75535</td>\n",
              "      <td>-61.05473</td>\n",
              "      <td>56.65182</td>\n",
              "      <td>15.29965</td>\n",
              "      <td>95.88193</td>\n",
              "      <td>-10.63242</td>\n",
              "      <td>12.96552</td>\n",
              "      <td>92.11633</td>\n",
              "      <td>10.88815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515342</th>\n",
              "      <td>2006</td>\n",
              "      <td>45.12852</td>\n",
              "      <td>12.65758</td>\n",
              "      <td>-38.72018</td>\n",
              "      <td>8.80882</td>\n",
              "      <td>-29.29985</td>\n",
              "      <td>-2.28706</td>\n",
              "      <td>-18.40424</td>\n",
              "      <td>-22.28726</td>\n",
              "      <td>-4.52429</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.73598</td>\n",
              "      <td>-71.15954</td>\n",
              "      <td>-123.98443</td>\n",
              "      <td>121.26989</td>\n",
              "      <td>10.89629</td>\n",
              "      <td>34.62409</td>\n",
              "      <td>-248.61020</td>\n",
              "      <td>-6.07171</td>\n",
              "      <td>53.96319</td>\n",
              "      <td>-8.09364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515343</th>\n",
              "      <td>2006</td>\n",
              "      <td>44.16614</td>\n",
              "      <td>32.38368</td>\n",
              "      <td>-3.34971</td>\n",
              "      <td>-2.49165</td>\n",
              "      <td>-19.59278</td>\n",
              "      <td>-18.67098</td>\n",
              "      <td>8.78428</td>\n",
              "      <td>4.02039</td>\n",
              "      <td>-12.01230</td>\n",
              "      <td>...</td>\n",
              "      <td>67.16763</td>\n",
              "      <td>282.77624</td>\n",
              "      <td>-4.63677</td>\n",
              "      <td>144.00125</td>\n",
              "      <td>21.62652</td>\n",
              "      <td>-29.72432</td>\n",
              "      <td>71.47198</td>\n",
              "      <td>20.32240</td>\n",
              "      <td>14.83107</td>\n",
              "      <td>39.74909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515344</th>\n",
              "      <td>2005</td>\n",
              "      <td>51.85726</td>\n",
              "      <td>59.11655</td>\n",
              "      <td>26.39436</td>\n",
              "      <td>-5.46030</td>\n",
              "      <td>-20.69012</td>\n",
              "      <td>-19.95528</td>\n",
              "      <td>-6.72771</td>\n",
              "      <td>2.29590</td>\n",
              "      <td>10.31018</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.50511</td>\n",
              "      <td>-69.18291</td>\n",
              "      <td>60.58456</td>\n",
              "      <td>28.64599</td>\n",
              "      <td>-4.39620</td>\n",
              "      <td>-64.56491</td>\n",
              "      <td>-45.61012</td>\n",
              "      <td>-5.51512</td>\n",
              "      <td>32.35602</td>\n",
              "      <td>12.17352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>515345 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b54e164-72f2-497f-94e4-91bc3d9eea1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b54e164-72f2-497f-94e4-91bc3d9eea1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b54e164-72f2-497f-94e4-91bc3d9eea1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugy7FZ8eoAd",
        "outputId": "46b97cdf-0f00-4b81-ccbd-a2ac31cb2911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    year      var1       var2      var3      var4      var5      var6  \\\n",
              "0      1  49.94357   21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1      1  48.73215   18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2      1  50.95714   31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3      1  48.24750   -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4      1  50.97020   42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "5      1  50.54767    0.31568  92.35066  22.38696 -25.51870 -19.04928   \n",
              "6      1  50.57546   33.17843  50.53517  11.55217 -27.24764  -8.78206   \n",
              "7      1  48.26892    8.97526  75.23158  24.04945 -16.02105 -14.09491   \n",
              "8      1  49.75468   33.99581  56.73846   2.89581  -2.92429 -26.44413   \n",
              "9      1  45.17809   46.34234 -40.65357  -2.47909   1.21253  -0.65302   \n",
              "10     1  39.13076  -23.01763 -36.20583   1.67519  -4.27101  13.01158   \n",
              "11     1  37.66498  -34.05910 -17.36060 -26.77781 -39.95119 -20.75000   \n",
              "12     1  26.51957 -148.15762 -13.30095  -7.25851  17.22029 -21.99439   \n",
              "13     1  37.68491  -26.84185 -27.10566 -14.95883  -5.87200 -21.68979   \n",
              "14     0  39.11695   -8.29767 -51.37966  -4.42668 -30.06506 -11.95916   \n",
              "15     1  35.05129  -67.97714 -14.20239  -6.68696  -0.61230 -18.70341   \n",
              "16     1  33.63129  -96.14912 -89.38216 -12.11699  13.77252  -6.69377   \n",
              "17     0  41.38639  -20.78665  51.80155  17.21415 -36.44189 -11.53169   \n",
              "18     0  37.45034   11.42615  56.28982  19.58426 -16.43530   2.22457   \n",
              "19     0  39.71092   -4.92800  12.88590 -11.87773   2.48031 -16.11028   \n",
              "\n",
              "        var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0  -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1    8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2   -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3    5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4  -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "5   20.67345  -5.19943   3.63566  ...   6.59753  -50.69577   26.02574   \n",
              "6  -12.04282  -9.53930  28.61811  ...  11.63681   25.44182  134.62382   \n",
              "7    8.11871  -1.87566   7.46701  ...  18.03989  -58.46192  -65.56438   \n",
              "8    1.71392  -0.55644  22.08594  ...  18.70812    5.20391  -27.75192   \n",
              "9   -6.95536 -12.20040  17.02512  ...  -4.36742  -87.55285  -70.79677   \n",
              "10   8.05718  -8.41088   6.27370  ...  32.86051  -26.08461 -186.82429   \n",
              "11  -0.10231  -0.89972  -1.30205  ...  11.18909   45.20614   53.83925   \n",
              "12   5.51947   3.48418   2.61738  ...  23.80442  251.76360   18.81642   \n",
              "13   4.87374 -18.01800   1.52141  ... -67.57637  234.27192  -72.34557   \n",
              "14  -0.85322  -8.86179  11.36680  ...  42.22923  478.26580  -10.33823   \n",
              "15  -1.31928  -9.46370   5.53492  ...  10.25585   94.90539   15.95689   \n",
              "16 -33.36843 -24.81437  21.22757  ...  49.93249  -14.47489   40.70590   \n",
              "17  11.75252  -7.62428  -3.65488  ...  50.37614  -40.48205   48.07805   \n",
              "18   1.02668  -7.34736  -0.01184  ... -22.46207  -25.77228 -322.42841   \n",
              "19 -16.40421  -8.29657   9.86817  ...  11.92816  -73.72412   16.19039   \n",
              "\n",
              "        var84     var85      var86      var87     var88       var89     var90  \n",
              "0    15.37344   1.11144  -23.08793   68.40795  -1.82223   -27.46348   2.26327  \n",
              "1    42.87836  -9.90378  -32.22788   70.49388  12.04941    58.43453  26.92061  \n",
              "2    10.93792  -0.07568   43.20130 -115.00698  -0.05859    39.67068  -0.66345  \n",
              "3   -46.67617 -12.51516   82.58061  -72.08993   9.90558   199.62971  18.85382  \n",
              "4   -17.72522  -1.49237   -7.50035   51.76631   7.88713    55.66926  28.74903  \n",
              "5    18.94430  -0.33730    6.09352   35.18381   5.00283   -11.02257   0.02263  \n",
              "6    21.51982   8.17570   35.46251   11.57736   4.50056    -4.62739   1.40192  \n",
              "7    46.99856  -4.09602   56.37650  -18.29975  -0.30633     3.98364  -3.72556  \n",
              "8    17.22100  -0.85210  -15.67150  -26.36257   5.48708    -9.13495   6.08680  \n",
              "9    76.57355  -7.71727    3.26926 -298.49845  11.49326   -89.21804 -15.09719  \n",
              "10  113.58176   9.28727   44.60282  158.00425  -2.59543   109.19723  23.36143  \n",
              "11    2.59467  -4.00958  -47.74886 -170.92864  -5.19009     8.83617  -7.16056  \n",
              "12  157.09656 -27.79449 -137.72740  115.28414  23.00230  -164.02536  51.54138  \n",
              "13 -362.25101 -25.55019  -89.08971 -891.58937  14.11648 -1030.99180  99.28967  \n",
              "14 -103.76858  39.19511  -98.76636 -122.81061  -2.14942  -211.48202 -12.81569  \n",
              "15  -98.15732  -9.64859  -93.52834  -95.82981  20.73063  -562.07671  43.44696  \n",
              "16   58.63692   8.81522   27.28474    5.78046   3.44539   259.10825  10.28525  \n",
              "17   -7.62399   6.51934  -30.46090  -53.87264   4.44627    58.16913  -0.02409  \n",
              "18 -146.57408  13.61588   92.22918 -439.80259  25.73235   157.22967  38.70617  \n",
              "19    9.79606   9.71693   -9.90907  -20.65851   2.34002   -31.57015   1.58400  \n",
              "\n",
              "[20 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b8eb72f3-deba-447e-9a4a-ed214e738dad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>...</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>...</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>...</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>...</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>39.13076</td>\n",
              "      <td>-23.01763</td>\n",
              "      <td>-36.20583</td>\n",
              "      <td>1.67519</td>\n",
              "      <td>-4.27101</td>\n",
              "      <td>13.01158</td>\n",
              "      <td>8.05718</td>\n",
              "      <td>-8.41088</td>\n",
              "      <td>6.27370</td>\n",
              "      <td>...</td>\n",
              "      <td>32.86051</td>\n",
              "      <td>-26.08461</td>\n",
              "      <td>-186.82429</td>\n",
              "      <td>113.58176</td>\n",
              "      <td>9.28727</td>\n",
              "      <td>44.60282</td>\n",
              "      <td>158.00425</td>\n",
              "      <td>-2.59543</td>\n",
              "      <td>109.19723</td>\n",
              "      <td>23.36143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>37.66498</td>\n",
              "      <td>-34.05910</td>\n",
              "      <td>-17.36060</td>\n",
              "      <td>-26.77781</td>\n",
              "      <td>-39.95119</td>\n",
              "      <td>-20.75000</td>\n",
              "      <td>-0.10231</td>\n",
              "      <td>-0.89972</td>\n",
              "      <td>-1.30205</td>\n",
              "      <td>...</td>\n",
              "      <td>11.18909</td>\n",
              "      <td>45.20614</td>\n",
              "      <td>53.83925</td>\n",
              "      <td>2.59467</td>\n",
              "      <td>-4.00958</td>\n",
              "      <td>-47.74886</td>\n",
              "      <td>-170.92864</td>\n",
              "      <td>-5.19009</td>\n",
              "      <td>8.83617</td>\n",
              "      <td>-7.16056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>26.51957</td>\n",
              "      <td>-148.15762</td>\n",
              "      <td>-13.30095</td>\n",
              "      <td>-7.25851</td>\n",
              "      <td>17.22029</td>\n",
              "      <td>-21.99439</td>\n",
              "      <td>5.51947</td>\n",
              "      <td>3.48418</td>\n",
              "      <td>2.61738</td>\n",
              "      <td>...</td>\n",
              "      <td>23.80442</td>\n",
              "      <td>251.76360</td>\n",
              "      <td>18.81642</td>\n",
              "      <td>157.09656</td>\n",
              "      <td>-27.79449</td>\n",
              "      <td>-137.72740</td>\n",
              "      <td>115.28414</td>\n",
              "      <td>23.00230</td>\n",
              "      <td>-164.02536</td>\n",
              "      <td>51.54138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>37.68491</td>\n",
              "      <td>-26.84185</td>\n",
              "      <td>-27.10566</td>\n",
              "      <td>-14.95883</td>\n",
              "      <td>-5.87200</td>\n",
              "      <td>-21.68979</td>\n",
              "      <td>4.87374</td>\n",
              "      <td>-18.01800</td>\n",
              "      <td>1.52141</td>\n",
              "      <td>...</td>\n",
              "      <td>-67.57637</td>\n",
              "      <td>234.27192</td>\n",
              "      <td>-72.34557</td>\n",
              "      <td>-362.25101</td>\n",
              "      <td>-25.55019</td>\n",
              "      <td>-89.08971</td>\n",
              "      <td>-891.58937</td>\n",
              "      <td>14.11648</td>\n",
              "      <td>-1030.99180</td>\n",
              "      <td>99.28967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>39.11695</td>\n",
              "      <td>-8.29767</td>\n",
              "      <td>-51.37966</td>\n",
              "      <td>-4.42668</td>\n",
              "      <td>-30.06506</td>\n",
              "      <td>-11.95916</td>\n",
              "      <td>-0.85322</td>\n",
              "      <td>-8.86179</td>\n",
              "      <td>11.36680</td>\n",
              "      <td>...</td>\n",
              "      <td>42.22923</td>\n",
              "      <td>478.26580</td>\n",
              "      <td>-10.33823</td>\n",
              "      <td>-103.76858</td>\n",
              "      <td>39.19511</td>\n",
              "      <td>-98.76636</td>\n",
              "      <td>-122.81061</td>\n",
              "      <td>-2.14942</td>\n",
              "      <td>-211.48202</td>\n",
              "      <td>-12.81569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>35.05129</td>\n",
              "      <td>-67.97714</td>\n",
              "      <td>-14.20239</td>\n",
              "      <td>-6.68696</td>\n",
              "      <td>-0.61230</td>\n",
              "      <td>-18.70341</td>\n",
              "      <td>-1.31928</td>\n",
              "      <td>-9.46370</td>\n",
              "      <td>5.53492</td>\n",
              "      <td>...</td>\n",
              "      <td>10.25585</td>\n",
              "      <td>94.90539</td>\n",
              "      <td>15.95689</td>\n",
              "      <td>-98.15732</td>\n",
              "      <td>-9.64859</td>\n",
              "      <td>-93.52834</td>\n",
              "      <td>-95.82981</td>\n",
              "      <td>20.73063</td>\n",
              "      <td>-562.07671</td>\n",
              "      <td>43.44696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>33.63129</td>\n",
              "      <td>-96.14912</td>\n",
              "      <td>-89.38216</td>\n",
              "      <td>-12.11699</td>\n",
              "      <td>13.77252</td>\n",
              "      <td>-6.69377</td>\n",
              "      <td>-33.36843</td>\n",
              "      <td>-24.81437</td>\n",
              "      <td>21.22757</td>\n",
              "      <td>...</td>\n",
              "      <td>49.93249</td>\n",
              "      <td>-14.47489</td>\n",
              "      <td>40.70590</td>\n",
              "      <td>58.63692</td>\n",
              "      <td>8.81522</td>\n",
              "      <td>27.28474</td>\n",
              "      <td>5.78046</td>\n",
              "      <td>3.44539</td>\n",
              "      <td>259.10825</td>\n",
              "      <td>10.28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>41.38639</td>\n",
              "      <td>-20.78665</td>\n",
              "      <td>51.80155</td>\n",
              "      <td>17.21415</td>\n",
              "      <td>-36.44189</td>\n",
              "      <td>-11.53169</td>\n",
              "      <td>11.75252</td>\n",
              "      <td>-7.62428</td>\n",
              "      <td>-3.65488</td>\n",
              "      <td>...</td>\n",
              "      <td>50.37614</td>\n",
              "      <td>-40.48205</td>\n",
              "      <td>48.07805</td>\n",
              "      <td>-7.62399</td>\n",
              "      <td>6.51934</td>\n",
              "      <td>-30.46090</td>\n",
              "      <td>-53.87264</td>\n",
              "      <td>4.44627</td>\n",
              "      <td>58.16913</td>\n",
              "      <td>-0.02409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>37.45034</td>\n",
              "      <td>11.42615</td>\n",
              "      <td>56.28982</td>\n",
              "      <td>19.58426</td>\n",
              "      <td>-16.43530</td>\n",
              "      <td>2.22457</td>\n",
              "      <td>1.02668</td>\n",
              "      <td>-7.34736</td>\n",
              "      <td>-0.01184</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.46207</td>\n",
              "      <td>-25.77228</td>\n",
              "      <td>-322.42841</td>\n",
              "      <td>-146.57408</td>\n",
              "      <td>13.61588</td>\n",
              "      <td>92.22918</td>\n",
              "      <td>-439.80259</td>\n",
              "      <td>25.73235</td>\n",
              "      <td>157.22967</td>\n",
              "      <td>38.70617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>39.71092</td>\n",
              "      <td>-4.92800</td>\n",
              "      <td>12.88590</td>\n",
              "      <td>-11.87773</td>\n",
              "      <td>2.48031</td>\n",
              "      <td>-16.11028</td>\n",
              "      <td>-16.40421</td>\n",
              "      <td>-8.29657</td>\n",
              "      <td>9.86817</td>\n",
              "      <td>...</td>\n",
              "      <td>11.92816</td>\n",
              "      <td>-73.72412</td>\n",
              "      <td>16.19039</td>\n",
              "      <td>9.79606</td>\n",
              "      <td>9.71693</td>\n",
              "      <td>-9.90907</td>\n",
              "      <td>-20.65851</td>\n",
              "      <td>2.34002</td>\n",
              "      <td>-31.57015</td>\n",
              "      <td>1.58400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b8eb72f3-deba-447e-9a4a-ed214e738dad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b8eb72f3-deba-447e-9a4a-ed214e738dad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b8eb72f3-deba-447e-9a4a-ed214e738dad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "Explain why it would be problematic to have\n",
        "some songs from an artist in the training set, and other songs from the same artist in the\n",
        "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
        "will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NiYlxpFzGrB"
      },
      "source": [
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()\n",
        "\n",
        "# Write your explanation here\n",
        "\n",
        "# Songs from the same artist might have the same features due to the artist style.\n",
        "# In order to test our model predictions for general data, test set must include data that wasn't in the train set, otherwise,\n",
        "# we might have an overfit due to the producer effect and it will seem like good results on the test because the model overfitted to similar data.\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
        "has the *same* mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "source": [
        "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
        "feature_stds  = df_train.std()[1:].to_numpy()\n",
        "\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zmZk6ezGrC"
      },
      "source": [
        "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
        "This is *not* a bug.\n",
        "\n",
        "Explain why it would be improper to compute and use test set means\n",
        "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZy6brwzGrC"
      },
      "source": [
        "# Write your explanation here\n",
        "\n",
        "# We will use our test to measure the model accuracy on new data (which might not be in a set/ batch) and will test it predictions according to the\n",
        "# learned parameters during the training.\n",
        "# during inference (predictions) we will use the mean and standard deviation of the train so in order for the test\n",
        "# to represent the model accuracy during inference it should use the same parameters.\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "\n",
        "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
        "set during the model building process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXv1U3gzGrC"
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
        "\n",
        "# Write your explanation here\n",
        "\n",
        "# in order for the test set to represent how well the model will perform on new data' we should be sure that we test it with a new data which wasn't a part\n",
        "# of the training considerations, hence, the test set should be used only once at the end of the process to verify the model accuracy.\n",
        "# due to the need of hyper parameters tunung and feature engineering during the model training process in order to get the best model,\n",
        "# we are splitting our train set to train and validation.\n",
        "# The validation set will be used as a test set for determining the hyper parameters and test set will be used for testing our final trained model when hyper parameters and\n",
        "# features are final\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification.\n",
        "These helper functions are written for you. All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6BA_s-kzGrD"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "def cross_entropy(t, y):\n",
        "  return -t.reshape(-1,) * np.log(y.reshape(-1,)) - (1 - t.reshape(-1,)) * np.log(1 - y.reshape(-1,))\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def get_accuracy(y, t):\n",
        "  acc = 0\n",
        "  N = 0\n",
        "  for i in range(len(y)):\n",
        "    N += 1\n",
        "    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
        "      acc += 1\n",
        "  return acc / N"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Write a function `pred` that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by: \n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naY5mT4_zGrD"
      },
      "source": [
        "def pred(w, b, X):\n",
        "  \"\"\"\n",
        "  Returns the prediction `y` of the target based on the weights `w` and scalar bias `b`.\n",
        "\n",
        "  Preconditions: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "                 np.shape(X) = (N, 90) for some N\n",
        "\n",
        "  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
        "  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last decimals\n",
        "  \"\"\"\n",
        "  # Your code goes here \n",
        "\n",
        "  return np.clip(sigmoid(np.dot(X, w) + b), np.finfo(float).eps, 1 - np.finfo(float).eps)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "Write a function `derivative_cost` that computes and returns the gradients \n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80bu7qmzGrE"
      },
      "source": [
        "def derivative_cost(X, y, t):\n",
        "  \"\"\"\n",
        "  Returns a tuple containing the gradients dLdw and dLdb.\n",
        "\n",
        "  Precondition: np.shape(X) == (N, 90) for some N\n",
        "                np.shape(y) == (N,)\n",
        "                np.shape(t) == (N,)\n",
        "\n",
        "  Postcondition: np.shape(dLdw) = (90,)\n",
        "           type(dLdb) = float\n",
        "  \"\"\"\n",
        "  # Your code goes here\n",
        "  \n",
        "  dldy = y.reshape(-1,)-t.reshape(-1,)\n",
        "  dLdw = np.dot(X.T, dldy) / X.shape[0]\n",
        "  dLdb = np.mean(dldy)\n",
        "  return (dLdw, dLdb)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPRGM3BjKe2"
      },
      "source": [
        "# **Explenation on Gradients**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHfmPVdsg0eX"
      },
      "source": [
        "**Add here an explaination on how the gradients are computed**:\n",
        "\n",
        "---\n",
        "##**Gradient Computation for w and b:**  \n",
        "####**First we will define our functions:**  \n",
        "**Loss:** $\\,\\,\\,L(CE) = \\frac{1}{N}\\sum_{n=1}^{N}Cross Entropy_{n} $\n",
        "\\\n",
        "**CrossEntropy:** $\\,\\,\\,CE(y, t) = -\\overbrace{t}^{Label} * \\ln({Ïƒ}) - (1-t)*\\ln({1-Ïƒ})$\\\n",
        "\\\n",
        "**Sigmoid:** $\\,\\,\\, Ïƒ(z) = \\frac{1}{1 + \\exp^{-z}}$\\\n",
        "\\\n",
        "**z:**$\\,\\,\\,z(x, w, b) = wx +b$\n",
        "\n",
        "\\\n",
        "Our loss function **_L_** derivative is:\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{N}\\sum_{n=1}^{N}(\\frac{\\partial CE_{n}}{\\partial w})$$\n",
        "\\\n",
        "From now on we will focus on CE derivatives, we will apply those derivitives on our loss finction at the end:\n",
        "\n",
        "$$ \\frac{\\partial CE}{\\partial w}= \\frac{\\partial CE}{\\partial Ïƒ}*\\frac{\\partial Ïƒ}{\\partial z}*\\frac{\\partial z}{\\partial w}$$\n",
        "\\\n",
        "$$\\frac{\\partial CE}{\\partial b}= \\frac{\\partial CE}{\\partial Ïƒ}*\\frac{\\partial Ïƒ}{\\partial z}*\\frac{\\partial z}{\\partial b}$$\n",
        "\n",
        "\\\n",
        "According to the equations on top we should look at each function partial derivative in order to calculate the Gradien Decent: \n",
        "\n",
        "We will start with $\\frac{\\partial CE}{\\partial Ïƒ}:$  \n",
        "\n",
        "$$\\frac{\\partial CE}{\\partial Ïƒ} = \\frac{\\partial}{\\partial Ïƒ}[-{t} * \\ln({Ïƒ}) - (1-t)*\\ln({1-Ïƒ})]\n",
        "\\\\\n",
        "= \\frac{\\partial}{\\partial Ïƒ}[-{t} * \\ln({Ïƒ})] - \\frac{\\partial}{\\partial Ïƒ}[(1-t)*\\ln({1-Ïƒ})]\n",
        "\\\\\n",
        " = -\\frac{t}{y} - (-\\frac{1-t}{1-y})\n",
        " \\\\ -\\frac{t}{y} +\\frac{1-t}{1-y}$$\n",
        "\n",
        "\\\n",
        " Now we will calculate $\\frac{\\partial Ïƒ}{\\partial z}$:  \n",
        " $$\\frac{\\partial Ïƒ}{\\partial z} = \\frac{\\partial}{\\partial z}[\\frac{1}{1 + \\exp^{-z}}] \n",
        " \\\\= -(\\frac{-\\exp^{-z}}{(1 + \\exp^{-z})^{2}}) \n",
        " \\\\= \\frac{\\exp^{-z}}{(1 + \\exp^{-z})^{2}}$$\n",
        " \\\\\n",
        " \\\\\n",
        "We can see that:  \n",
        "$$ Ïƒ(z) = \\frac{1}{1 + \\exp^{-z}}\n",
        "\\\\\n",
        " 1 + \\exp^{-z} = \\frac{1}{Ïƒ}\n",
        "\\\\\n",
        "\\exp^{-z} = \\frac{1}{Ïƒ} - 1$$\n",
        "\n",
        "\\\n",
        "hence: $$\\frac{\\partial Ïƒ}{\\partial z} = \\frac{\\exp^{-z}}{(1 + \\exp^{-z})^{2}} \n",
        "\\\\= Ïƒ^{2}*(\\frac{1}{Ïƒ} - 1) \\,,\\,\\,\\,\\forall\\,\\,\\, Ïƒ \\ne 0\n",
        "\\\\= Ïƒ*(1-Ïƒ)$$  \n",
        "Now we will look on $\\frac{\\partial z}{\\partial w}$ and $\\frac{\\partial z}{\\partial b}$:  \n",
        "$$$\\frac{\\partial z}{\\partial w} = \\frac{\\partial}{\\partial w}(w*x + b) = x\n",
        "\\\\ \\frac{\\partial z}{\\partial b} = \\frac{\\partial}{\\partial b}(w*x + b) = 1 $$\n",
        "\n",
        "\\\n",
        "Now we will use our calculated derivtives:\n",
        "$$\\frac{\\partial CE}{\\partial w} = (-\\frac{t}{Ïƒ} +\\frac{1-t}{1-Ïƒ})*Ïƒ*(1-Ïƒ)*x\n",
        "\\\\ =  (\\frac{-t*Ïƒ*(1-Ïƒ)}{Ïƒ} +\\frac{Ïƒ*(1-Ïƒ)(1-t)}{1-Ïƒ})*x\n",
        "\\\\ =(-t*(1-Ïƒ) + Ïƒ*(1-t))*x \n",
        "\\\\ = (-t + t*Ïƒ +Ïƒ - t*Ïƒ)*x\n",
        "\\\\ = (Ïƒ - t)*x$$\n",
        "\\\n",
        "and accorfong to the calculation above:  \n",
        "$$\\frac{\\partial CE}{\\partial b} =(Ïƒ - t)*1$$\n",
        "\\\n",
        "applying on our loss function:  \n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{N}\\sum_{n=1}^{N}((Ïƒ_{n} - t_{n})*x_{n})$$ \n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{N}\\sum_{n=1}^{N}(Ïƒ_{n} - t_{n})$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRTD-fozGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f7cc72-cbcc-42c6-ffdf-fe67295313ce"
      },
      "source": [
        "# Your code goes here\n",
        "\n",
        "h = 1e-09\n",
        "\n",
        "t = np.zeros(2,)\n",
        "X = np.ones([2, 90])\n",
        "w = np.random.normal(0,1, size=(90,))\n",
        "\n",
        "b = 1\n",
        "y = pred(w, b, X)\n",
        "y_b_plus = pred(w, b + h, X)\n",
        "\n",
        "cost_y = cost(y, t)\n",
        "cost_y_b_plus = cost(y_b_plus, t)\n",
        "\n",
        "r1 = (cost_y_b_plus - cost_y) / h\n",
        "r2 = derivative_cost(X, y, t)[1]\n",
        "print(\"The analytical results is: \", r1)\n",
        "print(\"The algorithm results is: \", r2)\n",
        "print(\"Gradient difference for w1 (analytical-algorithm): \", r1-r2)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is:  0.9952412227676176\n",
            "The algorithm results is:  0.9999863905223877\n",
            "Gradient difference for w1 (analytical-algorithm):  -0.004745167754770074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "### Part (d) -- 7%\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTsHgnPzGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27aad93d-987b-4a54-8ad0-2b03f22ea17c"
      },
      "source": [
        "# Your code goes here. You might find this below code helpful: but it's\n",
        "# up to you to figure out how/why, and how to modify the code\n",
        "\n",
        "h = 2e-7\n",
        "w = np.random.normal(0,0.001, size=(90,))\n",
        "t = train_ts[:100]\n",
        "X = train_norm_xs[:100]\n",
        "# X = np.random.normal(size=(100,90))\n",
        "# t = np.random.randint(0,2, size=(100,))\n",
        "\n",
        "b = np.random.randn(1,)[0]\n",
        "\n",
        "y = pred(w, b, X)\n",
        "cost_y = cost(y,t)\n",
        "\n",
        "cost_y_w = np.zeros(90)\n",
        "cost_y_w_plus = np.zeros(90)\n",
        "for i in range(0, len(w)):\n",
        "  w_plus = w.copy()\n",
        "  w_plus[i] = w_plus[i] + h\n",
        "  y2_w_plus = pred(w_plus, b, X)\n",
        "  cost_y_w[i] = cost_y\n",
        "  cost_y_w_plus[i] = cost(y2_w_plus, t)\n",
        "  \n",
        "r1 = (cost_y_w_plus - cost_y_w) / h\n",
        "r2 = derivative_cost(X, y, t)[0]\n",
        "\n",
        "print(\"The analytical results is:\", r1)\n",
        "print(\"The algorithm results is: \", r2)\n",
        "print(\"Gradient difference for w2 (analytical-algorithm): \", r1-r2)\n",
        "print(\"max difference between calculations:\", np.max(np.abs(r1-r2)))\n",
        "print(\"mean difference between calculations: \", np.mean(np.abs(r1-r2)))\n",
        "print(\"median difference between calculations: \", np.median(np.abs(r1-r2)))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is: [-0.21177219  0.0271802  -0.05869794  0.0137017   0.00829452  0.13071925\n",
            " -0.06810786  0.08771307  0.01932909  0.07386367 -0.04770042  0.09331505\n",
            "  0.04566832 -0.03809582  0.04442648  0.12221517 -0.04897385  0.07746526\n",
            " -0.01606711 -0.00381147 -0.01529888  0.02403921  0.08132324 -0.01389877\n",
            " -0.07897575  0.04481075 -0.10606068 -0.03721829  0.07720181  0.14229159\n",
            "  0.01868502  0.05236663 -0.09336367  0.1011089  -0.03227479  0.07269444\n",
            "  0.07133519  0.00872321 -0.08678619  0.01759239  0.07898706  0.01835755\n",
            " -0.04632556  0.0322185   0.04935077  0.05937669 -0.01611016  0.10040595\n",
            " -0.097736    0.06981604 -0.02979445  0.16308542 -0.10715412  0.07222382\n",
            " -0.0261888  -0.04046059 -0.00782799  0.12975697 -0.02483172  0.13063275\n",
            "  0.0498159  -0.05452404  0.12837082 -0.03611897 -0.119187    0.14276936\n",
            "  0.05406702  0.01048507 -0.01334838 -0.06776834  0.05414654  0.10237208\n",
            " -0.12661568 -0.03058173 -0.04808966 -0.03281532 -0.0483644   0.08750646\n",
            " -0.11855195  0.00928632  0.00702574 -0.00902326  0.01609371  0.09163855\n",
            " -0.05586876  0.07909636 -0.01221408  0.08762833 -0.06613194  0.02480676]\n",
            "The algorithm results is:  [-0.21177221  0.02718018 -0.05869796  0.01370167  0.0082945   0.13071923\n",
            " -0.06810789  0.08771304  0.01932908  0.07386365 -0.04770044  0.09331503\n",
            "  0.0456683  -0.03809585  0.04442646  0.12221514 -0.04897386  0.07746524\n",
            " -0.01606713 -0.0038115  -0.01529889  0.02403915  0.08132322 -0.01389879\n",
            " -0.07897578  0.04481073 -0.1060607  -0.03721831  0.0772018   0.14229156\n",
            "  0.018685    0.0523666  -0.09336372  0.10110888 -0.0322748   0.07269442\n",
            "  0.07133517  0.00872319 -0.08678625  0.01759236  0.07898704  0.01835754\n",
            " -0.04632559  0.03221848  0.04935076  0.05937667 -0.01611018  0.10040593\n",
            " -0.09773603  0.06981601 -0.02979447  0.16308536 -0.10715414  0.0722238\n",
            " -0.02618882 -0.04046062 -0.00782801  0.12975694 -0.02483173  0.13063268\n",
            "  0.04981587 -0.05452406  0.1283708  -0.03611898 -0.11918703  0.14276933\n",
            "  0.05406699  0.01048505 -0.0133484  -0.06776836  0.05414651  0.10237206\n",
            " -0.12661572 -0.03058175 -0.04808967 -0.03281533 -0.04836442  0.08750643\n",
            " -0.11855198  0.0092863   0.00702573 -0.00902328  0.01609369  0.09163853\n",
            " -0.05586878  0.07909634 -0.0122141   0.0876283  -0.06613195  0.02480675]\n",
            "Gradient difference for w2 (analytical-algorithm):  [2.66582983e-08 2.04167741e-08 1.76340577e-08 2.75514110e-08\n",
            " 2.19751133e-08 2.73934885e-08 2.41663125e-08 2.86394115e-08\n",
            " 1.54115786e-08 2.52716172e-08 2.25988949e-08 1.83247456e-08\n",
            " 2.52383213e-08 2.53075376e-08 1.93989720e-08 2.73859328e-08\n",
            " 1.54826048e-08 2.39191416e-08 1.93409876e-08 3.56147373e-08\n",
            " 1.60849606e-08 6.49618775e-08 2.40565839e-08 1.41744725e-08\n",
            " 2.77109645e-08 1.79339700e-08 2.32815759e-08 2.36620820e-08\n",
            " 1.62860112e-08 2.87108565e-08 1.77734988e-08 2.20527938e-08\n",
            " 4.44816470e-08 2.04809536e-08 1.98113978e-08 1.42806494e-08\n",
            " 2.76415879e-08 1.81821833e-08 6.09547361e-08 2.20191919e-08\n",
            " 1.83371760e-08 1.52138480e-08 2.82648638e-08 2.52666922e-08\n",
            " 1.23021548e-08 1.81890799e-08 2.38920062e-08 2.83844897e-08\n",
            " 2.52614351e-08 3.09860576e-08 2.17531040e-08 5.53704234e-08\n",
            " 2.62548672e-08 1.41547821e-08 2.03912243e-08 3.18987087e-08\n",
            " 1.99602075e-08 3.02700660e-08 1.96062013e-08 6.58795750e-08\n",
            " 3.12204070e-08 2.16775770e-08 2.38136252e-08 1.46069694e-08\n",
            " 2.92121806e-08 2.86340519e-08 2.92154687e-08 2.10170774e-08\n",
            " 1.82193632e-08 1.89054055e-08 2.80537108e-08 1.47008126e-08\n",
            " 4.28520981e-08 2.48797347e-08 1.74950253e-08 1.49672345e-08\n",
            " 1.65618925e-08 3.49553790e-08 2.74442555e-08 1.56791430e-08\n",
            " 1.56770871e-08 2.82645374e-08 2.34987491e-08 1.90275679e-08\n",
            " 1.55565160e-08 2.14509649e-08 1.79269911e-08 2.59587892e-08\n",
            " 1.41846690e-08 9.72978759e-09]\n",
            "max difference between calculations: 6.587957498971342e-08\n",
            "mean difference between calculations:  2.4325510736504787e-08\n",
            "median difference between calculations:  2.2325844332982125e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### Part (e) -- 7%\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent. \n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "source": [
        "def run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu=0.1, batch_size=100, max_iters=100):\n",
        "  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
        "  We use:\n",
        "    - train_norm_xs and train_ts as the training set\n",
        "    - val_norm_xs and val_ts as the test set\n",
        "    - mu as the learning rate\n",
        "    - (w0, b0) as the initial values of (w, b)\n",
        "\n",
        "  Precondition: np.shape(w0) == (90,)\n",
        "                type(b0) == float\n",
        " \n",
        "  Postcondition: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "  \"\"\"\n",
        "  w = w0.copy()\n",
        "  b = b0.copy()\n",
        "  iter = 0\n",
        "  history = {}\n",
        "  val_cost_history = []\n",
        "  val_acc_history = []\n",
        "\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    reindex = np.random.permutation(len(train_norm_xs))\n",
        "    train_norm_xs = train_norm_xs[reindex]\n",
        "    train_ts = train_ts[reindex]\n",
        "\n",
        "    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_norm_xs[i:(i + batch_size)]\n",
        "      t = train_ts[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      prediction = pred(w, b, X)\n",
        "\n",
        "      # update w and b\n",
        "      dLdw, dLdb = derivative_cost(X, prediction, t)\n",
        "      w -= dLdw*mu\n",
        "      b -= dLdb*mu\n",
        "\n",
        "      # increment the iteration count\n",
        "    iter += 1\n",
        "      # compute and print the *validation* loss and accuracy\n",
        "    if (iter % 10 == 0):\n",
        "      val_cost = 0\n",
        "      val_acc = 0\n",
        "      count = 0\n",
        "      for i in range(0, len(val_norm_xs), batch_size): # iterate over each minibatch\n",
        "        # minibatch that we are working with:\n",
        "        X = val_norm_xs[i:(i + batch_size)]\n",
        "        t = val_ts[i:(i + batch_size), 0]\n",
        "\n",
        "        val_prediction = pred(w, b, X)\n",
        "        val_cost += cost(val_prediction, t)\n",
        "        val_acc += get_accuracy(val_prediction, t)\n",
        "        count += 1\n",
        "\n",
        "      val_cost /= count\n",
        "      val_acc /= count\n",
        "      print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (\n",
        "              iter, val_acc * 100, val_cost))\n",
        "      val_cost_history.append(val_cost)\n",
        "      val_acc_history.append(val_acc)\n",
        "\n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "\n",
        "      # Think what parameters you should return for further use\n",
        "  history[\"val_cost\"] = val_cost_history\n",
        "  history[\"val_acc\"] = val_acc_history\n",
        "  history[\"learning_rate\"] = mu\n",
        "  history[\"batch_size\"] = batch_size\n",
        "\n",
        "  return history, (w, b)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### Part (f) -- 7%\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
        "Show that if the learning rate $\\mu$ is too small, then convergence is slow.\n",
        "Also, show that if $\\mu$ is too large, then the optimization algorirthm does not converge. The demonstration should be made using plots showing these effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE32Iqo6zGrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1305bf7c-cba7-4375-8656-8eb7ba35cc2f"
      },
      "source": [
        "# w0 = np.random.randn(90)\n",
        "w0 = np.random.normal(0, 0.001, size=(90))\n",
        "\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "# Write your code here\n",
        "print(\"Small mu: \")\n",
        "small_mu_history, parameters_small = run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu=3e-6, batch_size=100, max_iters=200)\n",
        "print(\"\\n\\n\", '='*200, \"\\n\\nLarge mu: \")\n",
        "large_mu_history, parameters_large = run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu=5, batch_size=100, max_iters=200)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small mu: \n",
            "Iter 10. [Val Acc 56%, Loss 0.731183]\n",
            "Iter 20. [Val Acc 56%, Loss 0.721262]\n",
            "Iter 30. [Val Acc 56%, Loss 0.712447]\n",
            "Iter 40. [Val Acc 56%, Loss 0.704538]\n",
            "Iter 50. [Val Acc 56%, Loss 0.697382]\n",
            "Iter 60. [Val Acc 56%, Loss 0.690860]\n",
            "Iter 70. [Val Acc 56%, Loss 0.684878]\n",
            "Iter 80. [Val Acc 56%, Loss 0.679365]\n",
            "Iter 90. [Val Acc 57%, Loss 0.674261]\n",
            "Iter 100. [Val Acc 57%, Loss 0.669521]\n",
            "Iter 110. [Val Acc 57%, Loss 0.665104]\n",
            "Iter 120. [Val Acc 57%, Loss 0.660979]\n",
            "Iter 130. [Val Acc 58%, Loss 0.657118]\n",
            "Iter 140. [Val Acc 58%, Loss 0.653497]\n",
            "Iter 150. [Val Acc 58%, Loss 0.650096]\n",
            "Iter 160. [Val Acc 58%, Loss 0.646896]\n",
            "Iter 170. [Val Acc 59%, Loss 0.643882]\n",
            "Iter 180. [Val Acc 59%, Loss 0.641039]\n",
            "Iter 190. [Val Acc 59%, Loss 0.638354]\n",
            "Iter 200. [Val Acc 60%, Loss 0.635816]\n",
            "\n",
            "\n",
            " ======================================================================================================================================================================================================== \n",
            "\n",
            "Large mu: \n",
            "Iter 10. [Val Acc 64%, Loss 1.453356]\n",
            "Iter 20. [Val Acc 61%, Loss 2.631364]\n",
            "Iter 30. [Val Acc 68%, Loss 1.264240]\n",
            "Iter 40. [Val Acc 68%, Loss 1.241821]\n",
            "Iter 50. [Val Acc 69%, Loss 1.420358]\n",
            "Iter 60. [Val Acc 67%, Loss 1.550355]\n",
            "Iter 70. [Val Acc 63%, Loss 2.018489]\n",
            "Iter 80. [Val Acc 67%, Loss 1.341818]\n",
            "Iter 90. [Val Acc 67%, Loss 1.348057]\n",
            "Iter 100. [Val Acc 66%, Loss 1.526510]\n",
            "Iter 110. [Val Acc 63%, Loss 2.152018]\n",
            "Iter 120. [Val Acc 67%, Loss 2.021361]\n",
            "Iter 130. [Val Acc 61%, Loss 2.006374]\n",
            "Iter 140. [Val Acc 65%, Loss 1.756993]\n",
            "Iter 150. [Val Acc 64%, Loss 1.732162]\n",
            "Iter 160. [Val Acc 64%, Loss 1.829706]\n",
            "Iter 170. [Val Acc 69%, Loss 1.478119]\n",
            "Iter 180. [Val Acc 60%, Loss 3.058963]\n",
            "Iter 190. [Val Acc 70%, Loss 1.445459]\n",
            "Iter 200. [Val Acc 60%, Loss 2.431061]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "def plot_acc_loss(acc_history, loss_history, iteration_resolution, lr, batch_size):\n",
        "  \"\"\"Plot the values of validation accuracy and loss from model train history lists.\n",
        "  We use:\n",
        "    - acc_historys as the accuracy history list\n",
        "    - loss_history as the loss history list\n",
        "    - iteration_resolution as the values iterations resolution\n",
        "    - lr is the learning rate used for traunung\n",
        "    - batch_size is the batch size used for training\n",
        "\n",
        "  Precondition: type(acc_history) == list(flaot)\n",
        " \n",
        "  Postcondition: type(loss_history) == list(flaot)\n",
        "  \"\"\"\n",
        "  iterations = range(iteration_resolution, len(acc_history)*iteration_resolution + iteration_resolution, 10)\n",
        "  \n",
        "  plt.figure(figsize=[32,6])\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(iterations[:], acc_history[:], \"b-\")\n",
        "  plt.title(\"Model Accuracy,  learning rate - {}, batch size = {}\".format(lr, batch_size))\n",
        "  plt.xlabel(\"iteration\", fontsize=14)\n",
        "  plt.ylabel(\"Validation Accuracy\", rotation=90, fontsize=14)\n",
        "  ax1 = plt.gca()\n",
        "  ax1.set(ylim=(0, 1))\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=[32,6])\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(iterations[:], loss_history[:], \"r-\")\n",
        "  plt.title(\"Model Accuracy,  learning rate - {}, batch size = {}\".format(lr, batch_size))\n",
        "  plt.xlabel(\"iteration\", fontsize=14)\n",
        "  plt.ylabel(\"Validation Loss\", rotation=90, fontsize=14)\n",
        "  ax2 = plt.gca()\n",
        "  ax2.set(ylim=(0, max(loss_history) +1))\n",
        "  plt.grid()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "3BGW2webqpY9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acc_loss(small_mu_history[\"val_acc\"], small_mu_history[\"val_cost\"], 10, small_mu_history[\"learning_rate\"], small_mu_history[\"batch_size\"])"
      ],
      "metadata": {
        "id": "4Ng2E7nTvtvw",
        "outputId": "635ef8cd-d344-4aa3-bed7-5169638c2d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2304x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2304x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acc_loss(large_mu_history[\"val_acc\"], large_mu_history[\"val_cost\"], 10, large_mu_history[\"learning_rate\"], large_mu_history[\"batch_size\"])\n"
      ],
      "metadata": {
        "id": "NvXTevDK18rY",
        "outputId": "9be8e672-04e0-4daf-99d9-a03f4e283a2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2304x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2304x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S44GmUZ12o-g"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "Our cost/ loss function is a positive parabola, whiel the leaning rate denotes \n",
        "the step for correction over each parameter in (w, b).  \n",
        "\n",
        "While we are using too small learning rate we will converge slowly which cuse the loss function to decrease very slow and the accuracy ro increase slowly either.  \n",
        "We can notice the small learning rate in the first plot while the loss decreasing too slow and the accuracy is increasing slowly either, which means the model converges toward loss function minima in very small steps.  \n",
        "\n",
        "While we are using too large learning rate we won't be able to converge because we will skip over the loss function minima back and forth with too large steps.\n",
        "We can notice the large learning rate in the second plot while both, loss and accuracy are jiterring around some value,thats means the the model's parameters are changing but due to the large learning rate the change cause a skip over the minima back and forth on each step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### Part (g) -- 7%\n",
        "\n",
        "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
        "the learning rate $\\mu$ and the batch size. Show plots demostrating good and bad behaviours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFOFSwgzGrI"
      },
      "source": [
        "\n",
        "def get_search_params():\n",
        "  batch_size_list = [50, 100, 500, 1000]\n",
        "  learning_rate_list = [0.01, 0.001, 1e-4, 5e-5]\n",
        "\n",
        "  return batch_size_list, learning_rate_list\n",
        "\n",
        "def get_best_model(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0):\n",
        "  # w0 = np.random.randn(90)\n",
        "  # b0 = np.random.randn(1)[0] \n",
        "  batch_sizes , learning_rates = get_search_params()\n",
        "  best_model = {}\n",
        "  current_model = {}\n",
        "  all_models = {}\n",
        "  best_val_acc = 0\n",
        "  count = 0\n",
        "  for learning_rate in learning_rates:\n",
        "    for  batch_size in batch_sizes:\n",
        "      temp_model_history, parameters = run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu=learning_rate, batch_size=batch_size, max_iters=350)\n",
        "      acc = temp_model_history[\"val_acc\"][-1]\n",
        "      loss = temp_model_history[\"val_cost\"][-1]\n",
        "      print(\"finished training model with: learning rate = {:f}, batch size = {:.0f}, val accuracy = {:.2f} %, val loss = {:f}\".format(\n",
        "              learning_rate, batch_size, (acc * 100), loss))\n",
        "      v = \"{}\".format(count)\n",
        "      current_model[\"history\"] = temp_model_history\n",
        "      current_model[\"parameters\"] = parameters_large\n",
        "      current_model[\"learning_rate\"] = learning_rate\n",
        "      current_model[\"batch_size\"] = batch_size\n",
        "      all_models[v] = current_model\n",
        "      count +=1\n",
        "      if temp_model_history[\"val_acc\"][-1] > best_val_acc:\n",
        "        best_val_acc = temp_model_history[\"val_acc\"][-1]\n",
        "        best_model[\"history\"] = temp_model_history\n",
        "        best_model[\"parameters\"] = parameters_large\n",
        "        best_model[\"learning_rate\"] = learning_rate\n",
        "        best_model[\"batch_size\"] = batch_size\n",
        "\n",
        "  print(\"best model found with learning rate = {}, and batch size = {}\".format(best_model[\"learning_rate\"], best_model[\"batch_size\"]))\n",
        "  return best_model, all_models\n",
        "\n",
        "# Write your code here\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# w0 = np.random.normal(0, 1, size=(90))\n",
        "w0 = np.random.randn(90)\n",
        "\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "best_model, all_models = get_best_model(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX_Fvi9RuwC_",
        "outputId": "d6bad116-8b71-438d-d12c-d39928070c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 10. [Val Acc 74%, Loss 0.557637]\n",
            "Iter 20. [Val Acc 73%, Loss 0.558565]\n",
            "Iter 30. [Val Acc 73%, Loss 0.557669]\n",
            "Iter 40. [Val Acc 73%, Loss 0.556887]\n",
            "Iter 50. [Val Acc 74%, Loss 0.557524]\n",
            "Iter 60. [Val Acc 74%, Loss 0.556926]\n",
            "Iter 70. [Val Acc 73%, Loss 0.557276]\n",
            "Iter 80. [Val Acc 74%, Loss 0.557444]\n",
            "Iter 90. [Val Acc 73%, Loss 0.557551]\n",
            "Iter 100. [Val Acc 73%, Loss 0.557241]\n",
            "Iter 110. [Val Acc 74%, Loss 0.557136]\n",
            "Iter 120. [Val Acc 73%, Loss 0.557337]\n",
            "Iter 130. [Val Acc 74%, Loss 0.556757]\n",
            "Iter 140. [Val Acc 73%, Loss 0.556960]\n",
            "Iter 150. [Val Acc 73%, Loss 0.557436]\n",
            "Iter 160. [Val Acc 73%, Loss 0.557248]\n",
            "Iter 170. [Val Acc 74%, Loss 0.556800]\n",
            "Iter 180. [Val Acc 74%, Loss 0.557234]\n",
            "Iter 190. [Val Acc 73%, Loss 0.557502]\n",
            "Iter 200. [Val Acc 73%, Loss 0.557570]\n",
            "Iter 210. [Val Acc 74%, Loss 0.557863]\n",
            "Iter 220. [Val Acc 73%, Loss 0.557691]\n",
            "Iter 230. [Val Acc 73%, Loss 0.557149]\n",
            "Iter 240. [Val Acc 74%, Loss 0.557726]\n",
            "Iter 250. [Val Acc 73%, Loss 0.557854]\n",
            "Iter 260. [Val Acc 74%, Loss 0.556889]\n",
            "Iter 270. [Val Acc 73%, Loss 0.557310]\n",
            "Iter 280. [Val Acc 73%, Loss 0.557380]\n",
            "Iter 290. [Val Acc 74%, Loss 0.558238]\n",
            "Iter 300. [Val Acc 74%, Loss 0.557594]\n",
            "Iter 310. [Val Acc 73%, Loss 0.557319]\n",
            "Iter 320. [Val Acc 73%, Loss 0.558780]\n",
            "Iter 330. [Val Acc 74%, Loss 0.557229]\n",
            "Iter 340. [Val Acc 73%, Loss 0.558234]\n",
            "Iter 350. [Val Acc 73%, Loss 0.557266]\n",
            "finished training model with: learning rate = 0.010000, batch size = 50, val accuracy = 73.37 %, val loss = 0.557266\n",
            "Iter 10. [Val Acc 73%, Loss 0.557014]\n",
            "Iter 20. [Val Acc 73%, Loss 0.557042]\n",
            "Iter 30. [Val Acc 74%, Loss 0.556771]\n",
            "Iter 40. [Val Acc 74%, Loss 0.556830]\n",
            "Iter 50. [Val Acc 73%, Loss 0.557305]\n",
            "Iter 60. [Val Acc 73%, Loss 0.557198]\n",
            "Iter 70. [Val Acc 73%, Loss 0.557232]\n",
            "Iter 80. [Val Acc 74%, Loss 0.556632]\n",
            "Iter 90. [Val Acc 73%, Loss 0.556507]\n",
            "Iter 100. [Val Acc 73%, Loss 0.556801]\n",
            "Iter 110. [Val Acc 73%, Loss 0.557034]\n",
            "Iter 120. [Val Acc 73%, Loss 0.556971]\n",
            "Iter 130. [Val Acc 73%, Loss 0.556792]\n",
            "Iter 140. [Val Acc 73%, Loss 0.556941]\n",
            "Iter 150. [Val Acc 74%, Loss 0.556752]\n",
            "Iter 160. [Val Acc 73%, Loss 0.556567]\n",
            "Iter 170. [Val Acc 74%, Loss 0.556799]\n",
            "Iter 180. [Val Acc 73%, Loss 0.557215]\n",
            "Iter 190. [Val Acc 73%, Loss 0.557061]\n",
            "Iter 200. [Val Acc 73%, Loss 0.556899]\n",
            "Iter 210. [Val Acc 73%, Loss 0.556924]\n",
            "Iter 220. [Val Acc 74%, Loss 0.556982]\n",
            "Iter 230. [Val Acc 74%, Loss 0.556792]\n",
            "Iter 240. [Val Acc 73%, Loss 0.557237]\n",
            "Iter 250. [Val Acc 74%, Loss 0.556561]\n",
            "Iter 260. [Val Acc 73%, Loss 0.556632]\n",
            "Iter 270. [Val Acc 73%, Loss 0.557134]\n",
            "Iter 280. [Val Acc 74%, Loss 0.557029]\n",
            "Iter 290. [Val Acc 73%, Loss 0.557048]\n",
            "Iter 300. [Val Acc 73%, Loss 0.556881]\n",
            "Iter 310. [Val Acc 73%, Loss 0.556837]\n",
            "Iter 320. [Val Acc 73%, Loss 0.556685]\n",
            "Iter 330. [Val Acc 73%, Loss 0.557245]\n",
            "Iter 340. [Val Acc 73%, Loss 0.557460]\n",
            "Iter 350. [Val Acc 73%, Loss 0.556881]\n",
            "finished training model with: learning rate = 0.010000, batch size = 100, val accuracy = 73.46 %, val loss = 0.556881\n",
            "Iter 10. [Val Acc 73%, Loss 0.573646]\n",
            "Iter 20. [Val Acc 73%, Loss 0.558280]\n",
            "Iter 30. [Val Acc 73%, Loss 0.556821]\n",
            "Iter 40. [Val Acc 73%, Loss 0.556623]\n",
            "Iter 50. [Val Acc 73%, Loss 0.556524]\n",
            "Iter 60. [Val Acc 73%, Loss 0.556693]\n",
            "Iter 70. [Val Acc 73%, Loss 0.556645]\n",
            "Iter 80. [Val Acc 73%, Loss 0.556584]\n",
            "Iter 90. [Val Acc 73%, Loss 0.556623]\n",
            "Iter 100. [Val Acc 73%, Loss 0.556618]\n",
            "Iter 110. [Val Acc 73%, Loss 0.556647]\n",
            "Iter 120. [Val Acc 73%, Loss 0.556630]\n",
            "Iter 130. [Val Acc 73%, Loss 0.556587]\n",
            "Iter 140. [Val Acc 73%, Loss 0.556564]\n",
            "Iter 150. [Val Acc 73%, Loss 0.556570]\n",
            "Iter 160. [Val Acc 73%, Loss 0.556632]\n",
            "Iter 170. [Val Acc 73%, Loss 0.556544]\n",
            "Iter 180. [Val Acc 73%, Loss 0.556744]\n",
            "Iter 190. [Val Acc 73%, Loss 0.556636]\n",
            "Iter 200. [Val Acc 73%, Loss 0.556633]\n",
            "Iter 210. [Val Acc 73%, Loss 0.556672]\n",
            "Iter 220. [Val Acc 73%, Loss 0.556721]\n",
            "Iter 230. [Val Acc 74%, Loss 0.556629]\n",
            "Iter 240. [Val Acc 73%, Loss 0.556585]\n",
            "Iter 250. [Val Acc 73%, Loss 0.556592]\n",
            "Iter 260. [Val Acc 74%, Loss 0.556529]\n",
            "Iter 270. [Val Acc 73%, Loss 0.556658]\n",
            "Iter 280. [Val Acc 74%, Loss 0.556596]\n",
            "Iter 290. [Val Acc 73%, Loss 0.556765]\n",
            "Iter 300. [Val Acc 73%, Loss 0.556669]\n",
            "Iter 310. [Val Acc 73%, Loss 0.556546]\n",
            "Iter 320. [Val Acc 74%, Loss 0.556632]\n",
            "Iter 330. [Val Acc 73%, Loss 0.556665]\n",
            "Iter 340. [Val Acc 73%, Loss 0.556698]\n",
            "Iter 350. [Val Acc 73%, Loss 0.556552]\n",
            "finished training model with: learning rate = 0.010000, batch size = 500, val accuracy = 73.48 %, val loss = 0.556552\n",
            "Iter 10. [Val Acc 70%, Loss 0.668457]\n",
            "Iter 20. [Val Acc 73%, Loss 0.573741]\n",
            "Iter 30. [Val Acc 73%, Loss 0.561434]\n",
            "Iter 40. [Val Acc 73%, Loss 0.558220]\n",
            "Iter 50. [Val Acc 73%, Loss 0.557155]\n",
            "Iter 60. [Val Acc 73%, Loss 0.556873]\n",
            "Iter 70. [Val Acc 73%, Loss 0.556594]\n",
            "Iter 80. [Val Acc 73%, Loss 0.556601]\n",
            "Iter 90. [Val Acc 73%, Loss 0.556584]\n",
            "Iter 100. [Val Acc 73%, Loss 0.556580]\n",
            "Iter 110. [Val Acc 73%, Loss 0.556568]\n",
            "Iter 120. [Val Acc 74%, Loss 0.556594]\n",
            "Iter 130. [Val Acc 73%, Loss 0.556575]\n",
            "Iter 140. [Val Acc 73%, Loss 0.556567]\n",
            "Iter 150. [Val Acc 73%, Loss 0.556650]\n",
            "Iter 160. [Val Acc 73%, Loss 0.556605]\n",
            "Iter 170. [Val Acc 73%, Loss 0.556545]\n",
            "Iter 180. [Val Acc 73%, Loss 0.556575]\n",
            "Iter 190. [Val Acc 73%, Loss 0.556579]\n",
            "Iter 200. [Val Acc 73%, Loss 0.556529]\n",
            "Iter 210. [Val Acc 73%, Loss 0.556620]\n",
            "Iter 220. [Val Acc 73%, Loss 0.556553]\n",
            "Iter 230. [Val Acc 74%, Loss 0.556509]\n",
            "Iter 240. [Val Acc 73%, Loss 0.556570]\n",
            "Iter 250. [Val Acc 73%, Loss 0.556562]\n",
            "Iter 260. [Val Acc 73%, Loss 0.556577]\n",
            "Iter 270. [Val Acc 73%, Loss 0.556669]\n",
            "Iter 280. [Val Acc 73%, Loss 0.556628]\n",
            "Iter 290. [Val Acc 73%, Loss 0.556551]\n",
            "Iter 300. [Val Acc 73%, Loss 0.556574]\n",
            "Iter 310. [Val Acc 73%, Loss 0.556627]\n",
            "Iter 320. [Val Acc 73%, Loss 0.556580]\n",
            "Iter 330. [Val Acc 73%, Loss 0.556596]\n",
            "Iter 340. [Val Acc 73%, Loss 0.556582]\n",
            "Iter 350. [Val Acc 73%, Loss 0.556615]\n",
            "finished training model with: learning rate = 0.010000, batch size = 1000, val accuracy = 73.47 %, val loss = 0.556615\n",
            "Iter 10. [Val Acc 73%, Loss 0.573609]\n",
            "Iter 20. [Val Acc 73%, Loss 0.558164]\n",
            "Iter 30. [Val Acc 73%, Loss 0.556869]\n",
            "Iter 40. [Val Acc 73%, Loss 0.556620]\n",
            "Iter 50. [Val Acc 73%, Loss 0.556545]\n",
            "Iter 60. [Val Acc 73%, Loss 0.556522]\n",
            "Iter 70. [Val Acc 74%, Loss 0.556630]\n",
            "Iter 80. [Val Acc 73%, Loss 0.556718]\n",
            "Iter 90. [Val Acc 73%, Loss 0.556593]\n",
            "Iter 100. [Val Acc 73%, Loss 0.556698]\n",
            "Iter 110. [Val Acc 74%, Loss 0.556554]\n",
            "Iter 120. [Val Acc 73%, Loss 0.556591]\n",
            "Iter 130. [Val Acc 73%, Loss 0.556600]\n",
            "Iter 140. [Val Acc 74%, Loss 0.556620]\n",
            "Iter 150. [Val Acc 74%, Loss 0.556573]\n",
            "Iter 160. [Val Acc 73%, Loss 0.556592]\n",
            "Iter 170. [Val Acc 73%, Loss 0.556654]\n",
            "Iter 180. [Val Acc 73%, Loss 0.556548]\n",
            "Iter 190. [Val Acc 73%, Loss 0.556552]\n",
            "Iter 200. [Val Acc 74%, Loss 0.556598]\n",
            "Iter 210. [Val Acc 73%, Loss 0.556560]\n",
            "Iter 220. [Val Acc 73%, Loss 0.556661]\n",
            "Iter 230. [Val Acc 73%, Loss 0.556587]\n",
            "Iter 240. [Val Acc 73%, Loss 0.556700]\n",
            "Iter 250. [Val Acc 73%, Loss 0.556634]\n",
            "Iter 260. [Val Acc 73%, Loss 0.556540]\n",
            "Iter 270. [Val Acc 74%, Loss 0.556525]\n",
            "Iter 280. [Val Acc 73%, Loss 0.556599]\n",
            "Iter 290. [Val Acc 73%, Loss 0.556506]\n",
            "Iter 300. [Val Acc 73%, Loss 0.556782]\n",
            "Iter 310. [Val Acc 73%, Loss 0.556607]\n",
            "Iter 320. [Val Acc 73%, Loss 0.556597]\n",
            "Iter 330. [Val Acc 73%, Loss 0.556626]\n",
            "Iter 340. [Val Acc 73%, Loss 0.556533]\n",
            "Iter 350. [Val Acc 73%, Loss 0.556650]\n",
            "finished training model with: learning rate = 0.001000, batch size = 50, val accuracy = 73.44 %, val loss = 0.556650\n",
            "Iter 10. [Val Acc 70%, Loss 0.667959]\n",
            "Iter 20. [Val Acc 73%, Loss 0.573660]\n",
            "Iter 30. [Val Acc 73%, Loss 0.561410]\n",
            "Iter 40. [Val Acc 73%, Loss 0.558226]\n",
            "Iter 50. [Val Acc 73%, Loss 0.557110]\n",
            "Iter 60. [Val Acc 73%, Loss 0.556810]\n",
            "Iter 70. [Val Acc 73%, Loss 0.556629]\n",
            "Iter 80. [Val Acc 74%, Loss 0.556597]\n",
            "Iter 90. [Val Acc 73%, Loss 0.556616]\n",
            "Iter 100. [Val Acc 73%, Loss 0.556622]\n",
            "Iter 110. [Val Acc 73%, Loss 0.556606]\n",
            "Iter 120. [Val Acc 74%, Loss 0.556545]\n",
            "Iter 130. [Val Acc 73%, Loss 0.556534]\n",
            "Iter 140. [Val Acc 73%, Loss 0.556619]\n",
            "Iter 150. [Val Acc 73%, Loss 0.556633]\n",
            "Iter 160. [Val Acc 73%, Loss 0.556593]\n",
            "Iter 170. [Val Acc 73%, Loss 0.556570]\n",
            "Iter 180. [Val Acc 73%, Loss 0.556575]\n",
            "Iter 190. [Val Acc 73%, Loss 0.556607]\n",
            "Iter 200. [Val Acc 73%, Loss 0.556647]\n",
            "Iter 210. [Val Acc 74%, Loss 0.556571]\n",
            "Iter 220. [Val Acc 73%, Loss 0.556548]\n",
            "Iter 230. [Val Acc 73%, Loss 0.556547]\n",
            "Iter 240. [Val Acc 73%, Loss 0.556559]\n",
            "Iter 250. [Val Acc 73%, Loss 0.556568]\n",
            "Iter 260. [Val Acc 73%, Loss 0.556578]\n",
            "Iter 270. [Val Acc 73%, Loss 0.556561]\n",
            "Iter 280. [Val Acc 73%, Loss 0.556612]\n",
            "Iter 290. [Val Acc 73%, Loss 0.556630]\n",
            "Iter 300. [Val Acc 73%, Loss 0.556527]\n",
            "Iter 310. [Val Acc 73%, Loss 0.556582]\n",
            "Iter 320. [Val Acc 74%, Loss 0.556549]\n",
            "Iter 330. [Val Acc 73%, Loss 0.556623]\n",
            "Iter 340. [Val Acc 73%, Loss 0.556617]\n",
            "Iter 350. [Val Acc 73%, Loss 0.556620]\n",
            "finished training model with: learning rate = 0.001000, batch size = 100, val accuracy = 73.49 %, val loss = 0.556620\n",
            "Iter 10. [Val Acc 57%, Loss 1.632173]\n",
            "Iter 20. [Val Acc 62%, Loss 1.148285]\n",
            "Iter 30. [Val Acc 65%, Loss 0.880717]\n",
            "Iter 40. [Val Acc 68%, Loss 0.741237]\n",
            "Iter 50. [Val Acc 70%, Loss 0.668090]\n",
            "Iter 60. [Val Acc 71%, Loss 0.627461]\n",
            "Iter 70. [Val Acc 72%, Loss 0.603632]\n",
            "Iter 80. [Val Acc 72%, Loss 0.589063]\n",
            "Iter 90. [Val Acc 72%, Loss 0.579799]\n",
            "Iter 100. [Val Acc 73%, Loss 0.573690]\n",
            "Iter 110. [Val Acc 73%, Loss 0.569476]\n",
            "Iter 120. [Val Acc 73%, Loss 0.566493]\n",
            "Iter 130. [Val Acc 73%, Loss 0.564321]\n",
            "Iter 140. [Val Acc 73%, Loss 0.562684]\n",
            "Iter 150. [Val Acc 73%, Loss 0.561421]\n",
            "Iter 160. [Val Acc 73%, Loss 0.560466]\n",
            "Iter 170. [Val Acc 73%, Loss 0.559704]\n",
            "Iter 180. [Val Acc 73%, Loss 0.559087]\n",
            "Iter 190. [Val Acc 73%, Loss 0.558610]\n",
            "Iter 200. [Val Acc 73%, Loss 0.558221]\n",
            "Iter 210. [Val Acc 73%, Loss 0.557918]\n",
            "Iter 220. [Val Acc 73%, Loss 0.557656]\n",
            "Iter 230. [Val Acc 73%, Loss 0.557452]\n",
            "Iter 240. [Val Acc 73%, Loss 0.557287]\n",
            "Iter 250. [Val Acc 73%, Loss 0.557162]\n",
            "Iter 260. [Val Acc 73%, Loss 0.557044]\n",
            "Iter 270. [Val Acc 73%, Loss 0.556960]\n",
            "Iter 280. [Val Acc 73%, Loss 0.556893]\n",
            "Iter 290. [Val Acc 73%, Loss 0.556824]\n",
            "Iter 300. [Val Acc 73%, Loss 0.556784]\n",
            "Iter 310. [Val Acc 73%, Loss 0.556742]\n",
            "Iter 320. [Val Acc 73%, Loss 0.556703]\n",
            "Iter 330. [Val Acc 74%, Loss 0.556678]\n",
            "Iter 340. [Val Acc 73%, Loss 0.556653]\n",
            "Iter 350. [Val Acc 73%, Loss 0.556637]\n",
            "finished training model with: learning rate = 0.001000, batch size = 500, val accuracy = 73.49 %, val loss = 0.556637\n",
            "Iter 10. [Val Acc 55%, Loss 2.006988]\n",
            "Iter 20. [Val Acc 57%, Loss 1.632931]\n",
            "Iter 30. [Val Acc 60%, Loss 1.357141]\n",
            "Iter 40. [Val Acc 62%, Loss 1.149138]\n",
            "Iter 50. [Val Acc 64%, Loss 0.994295]\n",
            "Iter 60. [Val Acc 65%, Loss 0.881390]\n",
            "Iter 70. [Val Acc 67%, Loss 0.800104]\n",
            "Iter 80. [Val Acc 68%, Loss 0.741707]\n",
            "Iter 90. [Val Acc 69%, Loss 0.699441]\n",
            "Iter 100. [Val Acc 70%, Loss 0.668409]\n",
            "Iter 110. [Val Acc 70%, Loss 0.645242]\n",
            "Iter 120. [Val Acc 71%, Loss 0.627693]\n",
            "Iter 130. [Val Acc 71%, Loss 0.614230]\n",
            "Iter 140. [Val Acc 72%, Loss 0.603795]\n",
            "Iter 150. [Val Acc 72%, Loss 0.595628]\n",
            "Iter 160. [Val Acc 72%, Loss 0.589178]\n",
            "Iter 170. [Val Acc 72%, Loss 0.584032]\n",
            "Iter 180. [Val Acc 72%, Loss 0.579886]\n",
            "Iter 190. [Val Acc 73%, Loss 0.576510]\n",
            "Iter 200. [Val Acc 73%, Loss 0.573738]\n",
            "Iter 210. [Val Acc 73%, Loss 0.571448]\n",
            "Iter 220. [Val Acc 73%, Loss 0.569533]\n",
            "Iter 230. [Val Acc 73%, Loss 0.567916]\n",
            "Iter 240. [Val Acc 73%, Loss 0.566537]\n",
            "Iter 250. [Val Acc 73%, Loss 0.565368]\n",
            "Iter 260. [Val Acc 73%, Loss 0.564348]\n",
            "Iter 270. [Val Acc 73%, Loss 0.563474]\n",
            "Iter 280. [Val Acc 73%, Loss 0.562709]\n",
            "Iter 290. [Val Acc 73%, Loss 0.562041]\n",
            "Iter 300. [Val Acc 73%, Loss 0.561453]\n",
            "Iter 310. [Val Acc 73%, Loss 0.560937]\n",
            "Iter 320. [Val Acc 73%, Loss 0.560479]\n",
            "Iter 330. [Val Acc 73%, Loss 0.560072]\n",
            "Iter 340. [Val Acc 73%, Loss 0.559712]\n",
            "Iter 350. [Val Acc 73%, Loss 0.559387]\n",
            "finished training model with: learning rate = 0.001000, batch size = 1000, val accuracy = 73.31 %, val loss = 0.559387\n",
            "Iter 10. [Val Acc 57%, Loss 1.631849]\n",
            "Iter 20. [Val Acc 62%, Loss 1.147911]\n",
            "Iter 30. [Val Acc 65%, Loss 0.880420]\n",
            "Iter 40. [Val Acc 68%, Loss 0.741033]\n",
            "Iter 50. [Val Acc 70%, Loss 0.667944]\n",
            "Iter 60. [Val Acc 71%, Loss 0.627375]\n",
            "Iter 70. [Val Acc 72%, Loss 0.603573]\n",
            "Iter 80. [Val Acc 72%, Loss 0.589018]\n",
            "Iter 90. [Val Acc 72%, Loss 0.579755]\n",
            "Iter 100. [Val Acc 73%, Loss 0.573664]\n",
            "Iter 110. [Val Acc 73%, Loss 0.569470]\n",
            "Iter 120. [Val Acc 73%, Loss 0.566482]\n",
            "Iter 130. [Val Acc 73%, Loss 0.564311]\n",
            "Iter 140. [Val Acc 73%, Loss 0.562665]\n",
            "Iter 150. [Val Acc 73%, Loss 0.561432]\n",
            "Iter 160. [Val Acc 73%, Loss 0.560451]\n",
            "Iter 170. [Val Acc 73%, Loss 0.559686]\n",
            "Iter 180. [Val Acc 73%, Loss 0.559083]\n",
            "Iter 190. [Val Acc 73%, Loss 0.558600]\n",
            "Iter 200. [Val Acc 73%, Loss 0.558213]\n",
            "Iter 210. [Val Acc 73%, Loss 0.557924]\n",
            "Iter 220. [Val Acc 73%, Loss 0.557645]\n",
            "Iter 230. [Val Acc 73%, Loss 0.557455]\n",
            "Iter 240. [Val Acc 73%, Loss 0.557280]\n",
            "Iter 250. [Val Acc 73%, Loss 0.557147]\n",
            "Iter 260. [Val Acc 73%, Loss 0.557047]\n",
            "Iter 270. [Val Acc 73%, Loss 0.556952]\n",
            "Iter 280. [Val Acc 73%, Loss 0.556894]\n",
            "Iter 290. [Val Acc 73%, Loss 0.556831]\n",
            "Iter 300. [Val Acc 73%, Loss 0.556792]\n",
            "Iter 310. [Val Acc 73%, Loss 0.556734]\n",
            "Iter 320. [Val Acc 73%, Loss 0.556703]\n",
            "Iter 330. [Val Acc 74%, Loss 0.556683]\n",
            "Iter 340. [Val Acc 73%, Loss 0.556662]\n",
            "Iter 350. [Val Acc 73%, Loss 0.556646]\n",
            "finished training model with: learning rate = 0.000100, batch size = 50, val accuracy = 73.49 %, val loss = 0.556646\n",
            "Iter 10. [Val Acc 55%, Loss 2.006227]\n",
            "Iter 20. [Val Acc 57%, Loss 1.631854]\n",
            "Iter 30. [Val Acc 60%, Loss 1.355924]\n",
            "Iter 40. [Val Acc 62%, Loss 1.147914]\n",
            "Iter 50. [Val Acc 64%, Loss 0.993183]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_history = best_model[\"history\"]\n",
        "plot_acc_loss(best_model_history[\"val_acc\"], best_model_history[\"val_cost\"], 10, best_model_history[\"learning_rate\"], best_model_history[\"batch_size\"])"
      ],
      "metadata": {
        "id": "_P5245t0u13N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZt7_932zX2"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### Part (h) -- 15%\n",
        "\n",
        "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKw2mLozGrI"
      },
      "source": [
        "# Write your code here\n",
        "w, b = best_model['parameters']\n",
        "\n",
        "train_acc = get_accuracy(pred(w, b, train_norm_xs), train_ts)\n",
        "val_acc = get_accuracy(pred(w, b, val_norm_xs), val_ts)\n",
        "test_acc = get_accuracy(pred(w, b, test_norm_xs), test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZa1u6920M3"
      },
      "source": [
        "**Explain and discuss your results here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4eP2Yh1zGrI"
      },
      "source": [
        "### Part (i) -- 15%\n",
        "\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
        "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "Compute the training, validation and test accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LCfAa1zGrJ"
      },
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "model = sklearn.linear_model.LogisticRegression()\n",
        "model.fit(train_xs, train_ts)\n",
        "\n",
        "train_acc = get_accuracy(model.predict(train_xs), train_ts)\n",
        "val_acc = get_accuracy(model.predict(val_xs), val_ts)\n",
        "test_acc = get_accuracy(model.predict(test_xs), test_ts)\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqucdV923tG"
      },
      "source": [
        "**This parts helps by checking if the code worked.**\n",
        "**Check if you get similar results, if not repair your code**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fqMU9AGl0Ob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}