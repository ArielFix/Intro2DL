{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArielFix/Intro2DL/blob/OnGoingAssignment1/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bhfIKbQzGq2"
      },
      "source": [
        "# Assignment 1. Music Century Classification\n",
        "\n",
        "**Assignment Responsible**: Natalie Lang.\n",
        "\n",
        "In this assignment, we will build models to predict which\n",
        "**century** a piece of music was released.  We will be using the \"YearPredictionMSD Data Set\"\n",
        "based on the Million Song Dataset. The data is available to download from the UCI \n",
        "Machine Learning Repository. Here are some links about the data:\n",
        "\n",
        "- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n",
        "- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n",
        "\n",
        "Note that you are note allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47oq1vy5PUIV"
      },
      "source": [
        "## Question 1. Data (21%)\n",
        "\n",
        "Start by setting up a Google Colab notebook in which to do your work.\n",
        "Since you are working with a partner, you might find this link helpful:\n",
        "\n",
        "- https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "\n",
        "The recommended way to work together is pair coding, where you and your partner are sitting together and writing code together. \n",
        "\n",
        "To process and read the data, we use the popular `pandas` package for data analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aFWpuNSzGq9"
      },
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7UWL6mFzGq-"
      },
      "source": [
        "Now that your notebook is set up, we can load the data into the notebook. The code below provides\n",
        "two ways of loading the data: directly from the internet, or through mounting Google Drive.\n",
        "The first method is easier but slower, and the second method is a bit involved at first, but\n",
        "can save you time later on. You will need to mount Google Drive for later assignments, so we recommend\n",
        "figuring how to do that now.\n",
        "\n",
        "Here are some resources to help you get started:\n",
        "\n",
        "- http.://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY6PrfV4zGq_",
        "outputId": "627bc4c6-f2bb-4686-ae49-f4a1c72e0a5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "load_from_drive = True\n",
        "\n",
        "if not load_from_drive:\n",
        "  csv_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\"\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  csv_path = '/content/gdrive/My Drive/MSc/Courses/Into to Deep Learnig/Assignments/YearPredictionMSD.txt.zip' # TODO - UPDATE ME WITH THE TRUE PATH!\n",
        "\n",
        "t_label = [\"year\"]\n",
        "x_labels = [\"var%d\" % i for i in range(1, 91)]\n",
        "df = pandas.read_csv(csv_path, names=t_label + x_labels)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgB83beNzGq_"
      },
      "source": [
        "Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n",
        "DataFrame `df` as a table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5bBEnj3zGq_",
        "outputId": "6d48bc65-fd64-4ba9-e2af-4beb307a84e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        year      var1      var2      var3      var4      var5      var6  \\\n",
              "0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "...      ...       ...       ...       ...       ...       ...       ...   \n",
              "515340  2006  51.28467  45.88068  22.19582  -5.53319  -3.61835 -16.36914   \n",
              "515341  2006  49.87870  37.93125  18.65987  -3.63581 -27.75665 -18.52988   \n",
              "515342  2006  45.12852  12.65758 -38.72018   8.80882 -29.29985  -2.28706   \n",
              "515343  2006  44.16614  32.38368  -3.34971  -2.49165 -19.59278 -18.67098   \n",
              "515344  2005  51.85726  59.11655  26.39436  -5.46030 -20.69012 -19.95528   \n",
              "\n",
              "            var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "...          ...       ...       ...  ...       ...        ...        ...   \n",
              "515340   2.12652   5.18160  -8.66890  ...   4.81440   -3.75991  -30.92584   \n",
              "515341   7.76108   3.56109  -2.50351  ...  32.38589  -32.75535  -61.05473   \n",
              "515342 -18.40424 -22.28726  -4.52429  ... -18.73598  -71.15954 -123.98443   \n",
              "515343   8.78428   4.02039 -12.01230  ...  67.16763  282.77624   -4.63677   \n",
              "515344  -6.72771   2.29590  10.31018  ... -11.50511  -69.18291   60.58456   \n",
              "\n",
              "            var84     var85     var86      var87     var88      var89  \\\n",
              "0        15.37344   1.11144 -23.08793   68.40795  -1.82223  -27.46348   \n",
              "1        42.87836  -9.90378 -32.22788   70.49388  12.04941   58.43453   \n",
              "2        10.93792  -0.07568  43.20130 -115.00698  -0.05859   39.67068   \n",
              "3       -46.67617 -12.51516  82.58061  -72.08993   9.90558  199.62971   \n",
              "4       -17.72522  -1.49237  -7.50035   51.76631   7.88713   55.66926   \n",
              "...           ...       ...       ...        ...       ...        ...   \n",
              "515340   26.33968  -5.03390  21.86037 -142.29410   3.42901  -41.14721   \n",
              "515341   56.65182  15.29965  95.88193  -10.63242  12.96552   92.11633   \n",
              "515342  121.26989  10.89629  34.62409 -248.61020  -6.07171   53.96319   \n",
              "515343  144.00125  21.62652 -29.72432   71.47198  20.32240   14.83107   \n",
              "515344   28.64599  -4.39620 -64.56491  -45.61012  -5.51512   32.35602   \n",
              "\n",
              "           var90  \n",
              "0        2.26327  \n",
              "1       26.92061  \n",
              "2       -0.66345  \n",
              "3       18.85382  \n",
              "4       28.74903  \n",
              "...          ...  \n",
              "515340 -15.46052  \n",
              "515341  10.88815  \n",
              "515342  -8.09364  \n",
              "515343  39.74909  \n",
              "515344  12.17352  \n",
              "\n",
              "[515345 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4ffe83f-2cb5-47ee-b604-689833acdb42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2001</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2001</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2001</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515340</th>\n",
              "      <td>2006</td>\n",
              "      <td>51.28467</td>\n",
              "      <td>45.88068</td>\n",
              "      <td>22.19582</td>\n",
              "      <td>-5.53319</td>\n",
              "      <td>-3.61835</td>\n",
              "      <td>-16.36914</td>\n",
              "      <td>2.12652</td>\n",
              "      <td>5.18160</td>\n",
              "      <td>-8.66890</td>\n",
              "      <td>...</td>\n",
              "      <td>4.81440</td>\n",
              "      <td>-3.75991</td>\n",
              "      <td>-30.92584</td>\n",
              "      <td>26.33968</td>\n",
              "      <td>-5.03390</td>\n",
              "      <td>21.86037</td>\n",
              "      <td>-142.29410</td>\n",
              "      <td>3.42901</td>\n",
              "      <td>-41.14721</td>\n",
              "      <td>-15.46052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515341</th>\n",
              "      <td>2006</td>\n",
              "      <td>49.87870</td>\n",
              "      <td>37.93125</td>\n",
              "      <td>18.65987</td>\n",
              "      <td>-3.63581</td>\n",
              "      <td>-27.75665</td>\n",
              "      <td>-18.52988</td>\n",
              "      <td>7.76108</td>\n",
              "      <td>3.56109</td>\n",
              "      <td>-2.50351</td>\n",
              "      <td>...</td>\n",
              "      <td>32.38589</td>\n",
              "      <td>-32.75535</td>\n",
              "      <td>-61.05473</td>\n",
              "      <td>56.65182</td>\n",
              "      <td>15.29965</td>\n",
              "      <td>95.88193</td>\n",
              "      <td>-10.63242</td>\n",
              "      <td>12.96552</td>\n",
              "      <td>92.11633</td>\n",
              "      <td>10.88815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515342</th>\n",
              "      <td>2006</td>\n",
              "      <td>45.12852</td>\n",
              "      <td>12.65758</td>\n",
              "      <td>-38.72018</td>\n",
              "      <td>8.80882</td>\n",
              "      <td>-29.29985</td>\n",
              "      <td>-2.28706</td>\n",
              "      <td>-18.40424</td>\n",
              "      <td>-22.28726</td>\n",
              "      <td>-4.52429</td>\n",
              "      <td>...</td>\n",
              "      <td>-18.73598</td>\n",
              "      <td>-71.15954</td>\n",
              "      <td>-123.98443</td>\n",
              "      <td>121.26989</td>\n",
              "      <td>10.89629</td>\n",
              "      <td>34.62409</td>\n",
              "      <td>-248.61020</td>\n",
              "      <td>-6.07171</td>\n",
              "      <td>53.96319</td>\n",
              "      <td>-8.09364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515343</th>\n",
              "      <td>2006</td>\n",
              "      <td>44.16614</td>\n",
              "      <td>32.38368</td>\n",
              "      <td>-3.34971</td>\n",
              "      <td>-2.49165</td>\n",
              "      <td>-19.59278</td>\n",
              "      <td>-18.67098</td>\n",
              "      <td>8.78428</td>\n",
              "      <td>4.02039</td>\n",
              "      <td>-12.01230</td>\n",
              "      <td>...</td>\n",
              "      <td>67.16763</td>\n",
              "      <td>282.77624</td>\n",
              "      <td>-4.63677</td>\n",
              "      <td>144.00125</td>\n",
              "      <td>21.62652</td>\n",
              "      <td>-29.72432</td>\n",
              "      <td>71.47198</td>\n",
              "      <td>20.32240</td>\n",
              "      <td>14.83107</td>\n",
              "      <td>39.74909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515344</th>\n",
              "      <td>2005</td>\n",
              "      <td>51.85726</td>\n",
              "      <td>59.11655</td>\n",
              "      <td>26.39436</td>\n",
              "      <td>-5.46030</td>\n",
              "      <td>-20.69012</td>\n",
              "      <td>-19.95528</td>\n",
              "      <td>-6.72771</td>\n",
              "      <td>2.29590</td>\n",
              "      <td>10.31018</td>\n",
              "      <td>...</td>\n",
              "      <td>-11.50511</td>\n",
              "      <td>-69.18291</td>\n",
              "      <td>60.58456</td>\n",
              "      <td>28.64599</td>\n",
              "      <td>-4.39620</td>\n",
              "      <td>-64.56491</td>\n",
              "      <td>-45.61012</td>\n",
              "      <td>-5.51512</td>\n",
              "      <td>32.35602</td>\n",
              "      <td>12.17352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>515345 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4ffe83f-2cb5-47ee-b604-689833acdb42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f4ffe83f-2cb5-47ee-b604-689833acdb42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f4ffe83f-2cb5-47ee-b604-689833acdb42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaLuAMH_zGrA"
      },
      "source": [
        "To set up our data for classification, we'll use the \"year\" field to represent\n",
        "whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n",
        "the year was released after 2000, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZdGlNgdzGrA"
      },
      "source": [
        "df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xugy7FZ8eoAd",
        "outputId": "6c1d2037-5359-4283-c2d3-62e4dd0beb34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        "df.head(20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    year      var1       var2      var3      var4      var5      var6  \\\n",
              "0      1  49.94357   21.47114  73.07750   8.74861 -17.40628 -13.09905   \n",
              "1      1  48.73215   18.42930  70.32679  12.94636 -10.32437 -24.83777   \n",
              "2      1  50.95714   31.85602  55.81851  13.41693  -6.57898 -18.54940   \n",
              "3      1  48.24750   -1.89837  36.29772   2.58776   0.97170 -26.21683   \n",
              "4      1  50.97020   42.20998  67.09964   8.46791 -15.85279 -16.81409   \n",
              "5      1  50.54767    0.31568  92.35066  22.38696 -25.51870 -19.04928   \n",
              "6      1  50.57546   33.17843  50.53517  11.55217 -27.24764  -8.78206   \n",
              "7      1  48.26892    8.97526  75.23158  24.04945 -16.02105 -14.09491   \n",
              "8      1  49.75468   33.99581  56.73846   2.89581  -2.92429 -26.44413   \n",
              "9      1  45.17809   46.34234 -40.65357  -2.47909   1.21253  -0.65302   \n",
              "10     1  39.13076  -23.01763 -36.20583   1.67519  -4.27101  13.01158   \n",
              "11     1  37.66498  -34.05910 -17.36060 -26.77781 -39.95119 -20.75000   \n",
              "12     1  26.51957 -148.15762 -13.30095  -7.25851  17.22029 -21.99439   \n",
              "13     1  37.68491  -26.84185 -27.10566 -14.95883  -5.87200 -21.68979   \n",
              "14     0  39.11695   -8.29767 -51.37966  -4.42668 -30.06506 -11.95916   \n",
              "15     1  35.05129  -67.97714 -14.20239  -6.68696  -0.61230 -18.70341   \n",
              "16     1  33.63129  -96.14912 -89.38216 -12.11699  13.77252  -6.69377   \n",
              "17     0  41.38639  -20.78665  51.80155  17.21415 -36.44189 -11.53169   \n",
              "18     0  37.45034   11.42615  56.28982  19.58426 -16.43530   2.22457   \n",
              "19     0  39.71092   -4.92800  12.88590 -11.87773   2.48031 -16.11028   \n",
              "\n",
              "        var7      var8      var9  ...     var81      var82      var83  \\\n",
              "0  -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n",
              "1    8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n",
              "2   -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n",
              "3    5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n",
              "4  -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n",
              "5   20.67345  -5.19943   3.63566  ...   6.59753  -50.69577   26.02574   \n",
              "6  -12.04282  -9.53930  28.61811  ...  11.63681   25.44182  134.62382   \n",
              "7    8.11871  -1.87566   7.46701  ...  18.03989  -58.46192  -65.56438   \n",
              "8    1.71392  -0.55644  22.08594  ...  18.70812    5.20391  -27.75192   \n",
              "9   -6.95536 -12.20040  17.02512  ...  -4.36742  -87.55285  -70.79677   \n",
              "10   8.05718  -8.41088   6.27370  ...  32.86051  -26.08461 -186.82429   \n",
              "11  -0.10231  -0.89972  -1.30205  ...  11.18909   45.20614   53.83925   \n",
              "12   5.51947   3.48418   2.61738  ...  23.80442  251.76360   18.81642   \n",
              "13   4.87374 -18.01800   1.52141  ... -67.57637  234.27192  -72.34557   \n",
              "14  -0.85322  -8.86179  11.36680  ...  42.22923  478.26580  -10.33823   \n",
              "15  -1.31928  -9.46370   5.53492  ...  10.25585   94.90539   15.95689   \n",
              "16 -33.36843 -24.81437  21.22757  ...  49.93249  -14.47489   40.70590   \n",
              "17  11.75252  -7.62428  -3.65488  ...  50.37614  -40.48205   48.07805   \n",
              "18   1.02668  -7.34736  -0.01184  ... -22.46207  -25.77228 -322.42841   \n",
              "19 -16.40421  -8.29657   9.86817  ...  11.92816  -73.72412   16.19039   \n",
              "\n",
              "        var84     var85      var86      var87     var88       var89     var90  \n",
              "0    15.37344   1.11144  -23.08793   68.40795  -1.82223   -27.46348   2.26327  \n",
              "1    42.87836  -9.90378  -32.22788   70.49388  12.04941    58.43453  26.92061  \n",
              "2    10.93792  -0.07568   43.20130 -115.00698  -0.05859    39.67068  -0.66345  \n",
              "3   -46.67617 -12.51516   82.58061  -72.08993   9.90558   199.62971  18.85382  \n",
              "4   -17.72522  -1.49237   -7.50035   51.76631   7.88713    55.66926  28.74903  \n",
              "5    18.94430  -0.33730    6.09352   35.18381   5.00283   -11.02257   0.02263  \n",
              "6    21.51982   8.17570   35.46251   11.57736   4.50056    -4.62739   1.40192  \n",
              "7    46.99856  -4.09602   56.37650  -18.29975  -0.30633     3.98364  -3.72556  \n",
              "8    17.22100  -0.85210  -15.67150  -26.36257   5.48708    -9.13495   6.08680  \n",
              "9    76.57355  -7.71727    3.26926 -298.49845  11.49326   -89.21804 -15.09719  \n",
              "10  113.58176   9.28727   44.60282  158.00425  -2.59543   109.19723  23.36143  \n",
              "11    2.59467  -4.00958  -47.74886 -170.92864  -5.19009     8.83617  -7.16056  \n",
              "12  157.09656 -27.79449 -137.72740  115.28414  23.00230  -164.02536  51.54138  \n",
              "13 -362.25101 -25.55019  -89.08971 -891.58937  14.11648 -1030.99180  99.28967  \n",
              "14 -103.76858  39.19511  -98.76636 -122.81061  -2.14942  -211.48202 -12.81569  \n",
              "15  -98.15732  -9.64859  -93.52834  -95.82981  20.73063  -562.07671  43.44696  \n",
              "16   58.63692   8.81522   27.28474    5.78046   3.44539   259.10825  10.28525  \n",
              "17   -7.62399   6.51934  -30.46090  -53.87264   4.44627    58.16913  -0.02409  \n",
              "18 -146.57408  13.61588   92.22918 -439.80259  25.73235   157.22967  38.70617  \n",
              "19    9.79606   9.71693   -9.90907  -20.65851   2.34002   -31.57015   1.58400  \n",
              "\n",
              "[20 rows x 91 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-82f6efc1-f6a7-41b7-8989-b44e9d702782\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>...</th>\n",
              "      <th>var81</th>\n",
              "      <th>var82</th>\n",
              "      <th>var83</th>\n",
              "      <th>var84</th>\n",
              "      <th>var85</th>\n",
              "      <th>var86</th>\n",
              "      <th>var87</th>\n",
              "      <th>var88</th>\n",
              "      <th>var89</th>\n",
              "      <th>var90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>49.94357</td>\n",
              "      <td>21.47114</td>\n",
              "      <td>73.07750</td>\n",
              "      <td>8.74861</td>\n",
              "      <td>-17.40628</td>\n",
              "      <td>-13.09905</td>\n",
              "      <td>-25.01202</td>\n",
              "      <td>-12.23257</td>\n",
              "      <td>7.83089</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01620</td>\n",
              "      <td>-54.40548</td>\n",
              "      <td>58.99367</td>\n",
              "      <td>15.37344</td>\n",
              "      <td>1.11144</td>\n",
              "      <td>-23.08793</td>\n",
              "      <td>68.40795</td>\n",
              "      <td>-1.82223</td>\n",
              "      <td>-27.46348</td>\n",
              "      <td>2.26327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>48.73215</td>\n",
              "      <td>18.42930</td>\n",
              "      <td>70.32679</td>\n",
              "      <td>12.94636</td>\n",
              "      <td>-10.32437</td>\n",
              "      <td>-24.83777</td>\n",
              "      <td>8.76630</td>\n",
              "      <td>-0.92019</td>\n",
              "      <td>18.76548</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66812</td>\n",
              "      <td>-19.68073</td>\n",
              "      <td>33.04964</td>\n",
              "      <td>42.87836</td>\n",
              "      <td>-9.90378</td>\n",
              "      <td>-32.22788</td>\n",
              "      <td>70.49388</td>\n",
              "      <td>12.04941</td>\n",
              "      <td>58.43453</td>\n",
              "      <td>26.92061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>50.95714</td>\n",
              "      <td>31.85602</td>\n",
              "      <td>55.81851</td>\n",
              "      <td>13.41693</td>\n",
              "      <td>-6.57898</td>\n",
              "      <td>-18.54940</td>\n",
              "      <td>-3.27872</td>\n",
              "      <td>-2.35035</td>\n",
              "      <td>16.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>3.03800</td>\n",
              "      <td>26.05866</td>\n",
              "      <td>-50.92779</td>\n",
              "      <td>10.93792</td>\n",
              "      <td>-0.07568</td>\n",
              "      <td>43.20130</td>\n",
              "      <td>-115.00698</td>\n",
              "      <td>-0.05859</td>\n",
              "      <td>39.67068</td>\n",
              "      <td>-0.66345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>48.24750</td>\n",
              "      <td>-1.89837</td>\n",
              "      <td>36.29772</td>\n",
              "      <td>2.58776</td>\n",
              "      <td>0.97170</td>\n",
              "      <td>-26.21683</td>\n",
              "      <td>5.05097</td>\n",
              "      <td>-10.34124</td>\n",
              "      <td>3.55005</td>\n",
              "      <td>...</td>\n",
              "      <td>34.57337</td>\n",
              "      <td>-171.70734</td>\n",
              "      <td>-16.96705</td>\n",
              "      <td>-46.67617</td>\n",
              "      <td>-12.51516</td>\n",
              "      <td>82.58061</td>\n",
              "      <td>-72.08993</td>\n",
              "      <td>9.90558</td>\n",
              "      <td>199.62971</td>\n",
              "      <td>18.85382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50.97020</td>\n",
              "      <td>42.20998</td>\n",
              "      <td>67.09964</td>\n",
              "      <td>8.46791</td>\n",
              "      <td>-15.85279</td>\n",
              "      <td>-16.81409</td>\n",
              "      <td>-12.48207</td>\n",
              "      <td>-9.37636</td>\n",
              "      <td>12.63699</td>\n",
              "      <td>...</td>\n",
              "      <td>9.92661</td>\n",
              "      <td>-55.95724</td>\n",
              "      <td>64.92712</td>\n",
              "      <td>-17.72522</td>\n",
              "      <td>-1.49237</td>\n",
              "      <td>-7.50035</td>\n",
              "      <td>51.76631</td>\n",
              "      <td>7.88713</td>\n",
              "      <td>55.66926</td>\n",
              "      <td>28.74903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>50.54767</td>\n",
              "      <td>0.31568</td>\n",
              "      <td>92.35066</td>\n",
              "      <td>22.38696</td>\n",
              "      <td>-25.51870</td>\n",
              "      <td>-19.04928</td>\n",
              "      <td>20.67345</td>\n",
              "      <td>-5.19943</td>\n",
              "      <td>3.63566</td>\n",
              "      <td>...</td>\n",
              "      <td>6.59753</td>\n",
              "      <td>-50.69577</td>\n",
              "      <td>26.02574</td>\n",
              "      <td>18.94430</td>\n",
              "      <td>-0.33730</td>\n",
              "      <td>6.09352</td>\n",
              "      <td>35.18381</td>\n",
              "      <td>5.00283</td>\n",
              "      <td>-11.02257</td>\n",
              "      <td>0.02263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>50.57546</td>\n",
              "      <td>33.17843</td>\n",
              "      <td>50.53517</td>\n",
              "      <td>11.55217</td>\n",
              "      <td>-27.24764</td>\n",
              "      <td>-8.78206</td>\n",
              "      <td>-12.04282</td>\n",
              "      <td>-9.53930</td>\n",
              "      <td>28.61811</td>\n",
              "      <td>...</td>\n",
              "      <td>11.63681</td>\n",
              "      <td>25.44182</td>\n",
              "      <td>134.62382</td>\n",
              "      <td>21.51982</td>\n",
              "      <td>8.17570</td>\n",
              "      <td>35.46251</td>\n",
              "      <td>11.57736</td>\n",
              "      <td>4.50056</td>\n",
              "      <td>-4.62739</td>\n",
              "      <td>1.40192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>48.26892</td>\n",
              "      <td>8.97526</td>\n",
              "      <td>75.23158</td>\n",
              "      <td>24.04945</td>\n",
              "      <td>-16.02105</td>\n",
              "      <td>-14.09491</td>\n",
              "      <td>8.11871</td>\n",
              "      <td>-1.87566</td>\n",
              "      <td>7.46701</td>\n",
              "      <td>...</td>\n",
              "      <td>18.03989</td>\n",
              "      <td>-58.46192</td>\n",
              "      <td>-65.56438</td>\n",
              "      <td>46.99856</td>\n",
              "      <td>-4.09602</td>\n",
              "      <td>56.37650</td>\n",
              "      <td>-18.29975</td>\n",
              "      <td>-0.30633</td>\n",
              "      <td>3.98364</td>\n",
              "      <td>-3.72556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>49.75468</td>\n",
              "      <td>33.99581</td>\n",
              "      <td>56.73846</td>\n",
              "      <td>2.89581</td>\n",
              "      <td>-2.92429</td>\n",
              "      <td>-26.44413</td>\n",
              "      <td>1.71392</td>\n",
              "      <td>-0.55644</td>\n",
              "      <td>22.08594</td>\n",
              "      <td>...</td>\n",
              "      <td>18.70812</td>\n",
              "      <td>5.20391</td>\n",
              "      <td>-27.75192</td>\n",
              "      <td>17.22100</td>\n",
              "      <td>-0.85210</td>\n",
              "      <td>-15.67150</td>\n",
              "      <td>-26.36257</td>\n",
              "      <td>5.48708</td>\n",
              "      <td>-9.13495</td>\n",
              "      <td>6.08680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>45.17809</td>\n",
              "      <td>46.34234</td>\n",
              "      <td>-40.65357</td>\n",
              "      <td>-2.47909</td>\n",
              "      <td>1.21253</td>\n",
              "      <td>-0.65302</td>\n",
              "      <td>-6.95536</td>\n",
              "      <td>-12.20040</td>\n",
              "      <td>17.02512</td>\n",
              "      <td>...</td>\n",
              "      <td>-4.36742</td>\n",
              "      <td>-87.55285</td>\n",
              "      <td>-70.79677</td>\n",
              "      <td>76.57355</td>\n",
              "      <td>-7.71727</td>\n",
              "      <td>3.26926</td>\n",
              "      <td>-298.49845</td>\n",
              "      <td>11.49326</td>\n",
              "      <td>-89.21804</td>\n",
              "      <td>-15.09719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1</td>\n",
              "      <td>39.13076</td>\n",
              "      <td>-23.01763</td>\n",
              "      <td>-36.20583</td>\n",
              "      <td>1.67519</td>\n",
              "      <td>-4.27101</td>\n",
              "      <td>13.01158</td>\n",
              "      <td>8.05718</td>\n",
              "      <td>-8.41088</td>\n",
              "      <td>6.27370</td>\n",
              "      <td>...</td>\n",
              "      <td>32.86051</td>\n",
              "      <td>-26.08461</td>\n",
              "      <td>-186.82429</td>\n",
              "      <td>113.58176</td>\n",
              "      <td>9.28727</td>\n",
              "      <td>44.60282</td>\n",
              "      <td>158.00425</td>\n",
              "      <td>-2.59543</td>\n",
              "      <td>109.19723</td>\n",
              "      <td>23.36143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1</td>\n",
              "      <td>37.66498</td>\n",
              "      <td>-34.05910</td>\n",
              "      <td>-17.36060</td>\n",
              "      <td>-26.77781</td>\n",
              "      <td>-39.95119</td>\n",
              "      <td>-20.75000</td>\n",
              "      <td>-0.10231</td>\n",
              "      <td>-0.89972</td>\n",
              "      <td>-1.30205</td>\n",
              "      <td>...</td>\n",
              "      <td>11.18909</td>\n",
              "      <td>45.20614</td>\n",
              "      <td>53.83925</td>\n",
              "      <td>2.59467</td>\n",
              "      <td>-4.00958</td>\n",
              "      <td>-47.74886</td>\n",
              "      <td>-170.92864</td>\n",
              "      <td>-5.19009</td>\n",
              "      <td>8.83617</td>\n",
              "      <td>-7.16056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>26.51957</td>\n",
              "      <td>-148.15762</td>\n",
              "      <td>-13.30095</td>\n",
              "      <td>-7.25851</td>\n",
              "      <td>17.22029</td>\n",
              "      <td>-21.99439</td>\n",
              "      <td>5.51947</td>\n",
              "      <td>3.48418</td>\n",
              "      <td>2.61738</td>\n",
              "      <td>...</td>\n",
              "      <td>23.80442</td>\n",
              "      <td>251.76360</td>\n",
              "      <td>18.81642</td>\n",
              "      <td>157.09656</td>\n",
              "      <td>-27.79449</td>\n",
              "      <td>-137.72740</td>\n",
              "      <td>115.28414</td>\n",
              "      <td>23.00230</td>\n",
              "      <td>-164.02536</td>\n",
              "      <td>51.54138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1</td>\n",
              "      <td>37.68491</td>\n",
              "      <td>-26.84185</td>\n",
              "      <td>-27.10566</td>\n",
              "      <td>-14.95883</td>\n",
              "      <td>-5.87200</td>\n",
              "      <td>-21.68979</td>\n",
              "      <td>4.87374</td>\n",
              "      <td>-18.01800</td>\n",
              "      <td>1.52141</td>\n",
              "      <td>...</td>\n",
              "      <td>-67.57637</td>\n",
              "      <td>234.27192</td>\n",
              "      <td>-72.34557</td>\n",
              "      <td>-362.25101</td>\n",
              "      <td>-25.55019</td>\n",
              "      <td>-89.08971</td>\n",
              "      <td>-891.58937</td>\n",
              "      <td>14.11648</td>\n",
              "      <td>-1030.99180</td>\n",
              "      <td>99.28967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>39.11695</td>\n",
              "      <td>-8.29767</td>\n",
              "      <td>-51.37966</td>\n",
              "      <td>-4.42668</td>\n",
              "      <td>-30.06506</td>\n",
              "      <td>-11.95916</td>\n",
              "      <td>-0.85322</td>\n",
              "      <td>-8.86179</td>\n",
              "      <td>11.36680</td>\n",
              "      <td>...</td>\n",
              "      <td>42.22923</td>\n",
              "      <td>478.26580</td>\n",
              "      <td>-10.33823</td>\n",
              "      <td>-103.76858</td>\n",
              "      <td>39.19511</td>\n",
              "      <td>-98.76636</td>\n",
              "      <td>-122.81061</td>\n",
              "      <td>-2.14942</td>\n",
              "      <td>-211.48202</td>\n",
              "      <td>-12.81569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1</td>\n",
              "      <td>35.05129</td>\n",
              "      <td>-67.97714</td>\n",
              "      <td>-14.20239</td>\n",
              "      <td>-6.68696</td>\n",
              "      <td>-0.61230</td>\n",
              "      <td>-18.70341</td>\n",
              "      <td>-1.31928</td>\n",
              "      <td>-9.46370</td>\n",
              "      <td>5.53492</td>\n",
              "      <td>...</td>\n",
              "      <td>10.25585</td>\n",
              "      <td>94.90539</td>\n",
              "      <td>15.95689</td>\n",
              "      <td>-98.15732</td>\n",
              "      <td>-9.64859</td>\n",
              "      <td>-93.52834</td>\n",
              "      <td>-95.82981</td>\n",
              "      <td>20.73063</td>\n",
              "      <td>-562.07671</td>\n",
              "      <td>43.44696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>33.63129</td>\n",
              "      <td>-96.14912</td>\n",
              "      <td>-89.38216</td>\n",
              "      <td>-12.11699</td>\n",
              "      <td>13.77252</td>\n",
              "      <td>-6.69377</td>\n",
              "      <td>-33.36843</td>\n",
              "      <td>-24.81437</td>\n",
              "      <td>21.22757</td>\n",
              "      <td>...</td>\n",
              "      <td>49.93249</td>\n",
              "      <td>-14.47489</td>\n",
              "      <td>40.70590</td>\n",
              "      <td>58.63692</td>\n",
              "      <td>8.81522</td>\n",
              "      <td>27.28474</td>\n",
              "      <td>5.78046</td>\n",
              "      <td>3.44539</td>\n",
              "      <td>259.10825</td>\n",
              "      <td>10.28525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>41.38639</td>\n",
              "      <td>-20.78665</td>\n",
              "      <td>51.80155</td>\n",
              "      <td>17.21415</td>\n",
              "      <td>-36.44189</td>\n",
              "      <td>-11.53169</td>\n",
              "      <td>11.75252</td>\n",
              "      <td>-7.62428</td>\n",
              "      <td>-3.65488</td>\n",
              "      <td>...</td>\n",
              "      <td>50.37614</td>\n",
              "      <td>-40.48205</td>\n",
              "      <td>48.07805</td>\n",
              "      <td>-7.62399</td>\n",
              "      <td>6.51934</td>\n",
              "      <td>-30.46090</td>\n",
              "      <td>-53.87264</td>\n",
              "      <td>4.44627</td>\n",
              "      <td>58.16913</td>\n",
              "      <td>-0.02409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>37.45034</td>\n",
              "      <td>11.42615</td>\n",
              "      <td>56.28982</td>\n",
              "      <td>19.58426</td>\n",
              "      <td>-16.43530</td>\n",
              "      <td>2.22457</td>\n",
              "      <td>1.02668</td>\n",
              "      <td>-7.34736</td>\n",
              "      <td>-0.01184</td>\n",
              "      <td>...</td>\n",
              "      <td>-22.46207</td>\n",
              "      <td>-25.77228</td>\n",
              "      <td>-322.42841</td>\n",
              "      <td>-146.57408</td>\n",
              "      <td>13.61588</td>\n",
              "      <td>92.22918</td>\n",
              "      <td>-439.80259</td>\n",
              "      <td>25.73235</td>\n",
              "      <td>157.22967</td>\n",
              "      <td>38.70617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>39.71092</td>\n",
              "      <td>-4.92800</td>\n",
              "      <td>12.88590</td>\n",
              "      <td>-11.87773</td>\n",
              "      <td>2.48031</td>\n",
              "      <td>-16.11028</td>\n",
              "      <td>-16.40421</td>\n",
              "      <td>-8.29657</td>\n",
              "      <td>9.86817</td>\n",
              "      <td>...</td>\n",
              "      <td>11.92816</td>\n",
              "      <td>-73.72412</td>\n",
              "      <td>16.19039</td>\n",
              "      <td>9.79606</td>\n",
              "      <td>9.71693</td>\n",
              "      <td>-9.90907</td>\n",
              "      <td>-20.65851</td>\n",
              "      <td>2.34002</td>\n",
              "      <td>-31.57015</td>\n",
              "      <td>1.58400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows Ã— 91 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-82f6efc1-f6a7-41b7-8989-b44e9d702782')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-82f6efc1-f6a7-41b7-8989-b44e9d702782 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-82f6efc1-f6a7-41b7-8989-b44e9d702782');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncjxI4WdzGrA"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "The data set description text asks us to respect the below train/test split to\n",
        "avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n",
        "ends up in both the training and test set.\n",
        "\n",
        "Explain why it would be problematic to have\n",
        "some songs from an artist in the training set, and other songs from the same artist in the\n",
        "test set. (Hint: Remember that we want our test accuracy to predict how well the model\n",
        "will perform in practice on a song it hasn't learned about.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NiYlxpFzGrB"
      },
      "source": [
        "df_train = df[:463715]\n",
        "df_test = df[463715:]\n",
        "\n",
        "# convert to numpy\n",
        "train_xs = df_train[x_labels].to_numpy()\n",
        "train_ts = df_train[t_label].to_numpy()\n",
        "test_xs = df_test[x_labels].to_numpy()\n",
        "test_ts = df_test[t_label].to_numpy()\n",
        "\n",
        "# Write your explanation here\n",
        "\n",
        "# Songs from the same artist might have the same features due to the artist style.\n",
        "# In order to test our model predictions for general data, test set must include data that wasn't in the train set, otherwise,\n",
        "# we might have an overfit due to the producer effect and it will seem like good results on the test because the model overfitted to similar data.\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYSzd4XUzGrB"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "It can be beneficial to **normalize** the columns, so that each column (feature)\n",
        "has the *same* mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPuWLksJzGrB"
      },
      "source": [
        "feature_means = df_train.mean()[1:].to_numpy() # the [1:] removes the mean of the \"year\" field\n",
        "feature_stds  = df_train.std()[1:].to_numpy()\n",
        "\n",
        "train_norm_xs = (train_xs - feature_means) / feature_stds\n",
        "test_norm_xs = (test_xs - feature_means) / feature_stds"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zmZk6ezGrC"
      },
      "source": [
        "Notice how in our code, we normalized the test set using the *training data means and standard deviations*.\n",
        "This is *not* a bug.\n",
        "\n",
        "Explain why it would be improper to compute and use test set means\n",
        "and standard deviations. (Hint: Remember what we want to use the test accuracy to measure.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZy6brwzGrC"
      },
      "source": [
        "# Write your explanation here\n",
        "\n",
        "# We will use our test to measure the model accuracy on new data (which might not be in a set/ batch) and will test it predictions according to the\n",
        "# learned parameters during the training.\n",
        "# during inference (predictions) we will use the mean and standard deviation of the train so in order for the test\n",
        "# to represent the model accuracy during inference it shoukd use the same parameters.\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GqL5J_zGrC"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "Finally, we'll move some of the data in our training set into a validation set.\n",
        "\n",
        "Explain why we should limit how many times we use the test set, and that we should use the validation\n",
        "set during the model building process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXv1U3gzGrC"
      },
      "source": [
        "# shuffle the training set\n",
        "reindex = np.random.permutation(len(train_xs))\n",
        "train_xs = train_xs[reindex]\n",
        "train_norm_xs = train_norm_xs[reindex]\n",
        "train_ts = train_ts[reindex]\n",
        "\n",
        "# use the first 50000 elements of `train_xs` as the validation set\n",
        "train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n",
        "train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n",
        "train_ts, val_ts           = train_ts[50000:], train_ts[:50000]\n",
        "\n",
        "# Write your explanation here\n",
        "\n",
        "# in order for the test set to represent how well the model will perform on new data' we should be sure that we test it with a new data which wasn't a part\n",
        "# of the training considerations, hence, the test set should be used only once at the end of the process to verify the model accuracy.\n",
        "# due to the need of hyper parameters tunung and feature engineering during the model training process in order to get the best model,\n",
        "# we are splitting our train set to train and validation.\n",
        "# The validation set will be used as a test set for determining the hyper parameters and test set will be used for testing our final trained model when hyper parameters and\n",
        "# features are final\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4lt445zGrD"
      },
      "source": [
        "## Part 2. Classification (79%)\n",
        "\n",
        "We will first build a *classification* model to perform decade classification.\n",
        "These helper functions are written for you. All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6BA_s-kzGrD"
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "def cross_entropy(t, y):\n",
        "  return -t * np.log(y + np.finfo(float).eps) - (1 - t) * np.log(1 - y + np.finfo(float).eps)\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n",
        "\n",
        "def get_accuracy(y, t):\n",
        "  acc = 0\n",
        "  N = 0\n",
        "  for i in range(len(y)):\n",
        "    N += 1\n",
        "    if (y[i] >= 0.5 and t[i] == 1) or (y[i] < 0.5 and t[i] == 0):\n",
        "      acc += 1\n",
        "  return acc / N"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ZIfooBzGrD"
      },
      "source": [
        "### Part (a) -- 7%\n",
        "\n",
        "Write a function `pred` that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by: \n",
        "\\begin{equation}\n",
        "y = \\sigma({\\bf w}^T {\\bf x} + b),\n",
        "\\end{equation}\n",
        "where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naY5mT4_zGrD"
      },
      "source": [
        "def pred(w, b, X):\n",
        "  \"\"\"\n",
        "  Returns the prediction `y` of the target based on the weights `w` and scalar bias `b`.\n",
        "\n",
        "  Preconditions: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "                 np.shape(X) = (N, 90) for some N\n",
        "\n",
        "  >>> pred(np.zeros(90), 1, np.ones([2, 90]))\n",
        "  array([0.73105858, 0.73105858]) # It's okay if your output differs in the last decimals\n",
        "  \"\"\"\n",
        "  # Your code goes here \n",
        "\n",
        "  return sigmoid(np.dot(X, w) + b)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxNdmSd3zGrE"
      },
      "source": [
        "### Part (b) -- 7%\n",
        "\n",
        "Write a function `derivative_cost` that computes and returns the gradients \n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n",
        "$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P80bu7qmzGrE"
      },
      "source": [
        "def derivative_cost(X, y, t):\n",
        "  \"\"\"\n",
        "  Returns a tuple containing the gradients dLdw and dLdb.\n",
        "\n",
        "  Precondition: np.shape(X) == (N, 90) for some N\n",
        "                np.shape(y) == (N,)\n",
        "                np.shape(t) == (N,)\n",
        "\n",
        "  Postcondition: np.shape(dLdw) = (90,)\n",
        "           type(dLdb) = float\n",
        "  \"\"\"\n",
        "  # Your code goes here\n",
        "  dldt = y-t\n",
        "  dLdw = np.dot(X.T, dldt) / X.shape[0]\n",
        "  dLdb = np.mean(dldt)\n",
        "  return (dLdw, dLdb)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okPRGM3BjKe2"
      },
      "source": [
        "# **Explenation on Gradients**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHfmPVdsg0eX"
      },
      "source": [
        "**Add here an explaination on how the gradients are computed**:\n",
        "\n",
        "---\n",
        "##Gradient Computation:  \n",
        "**First we will define our functions:  \n",
        " Loss Function - (L)  \n",
        " Cross Entropy - (CE)  \n",
        " Sigmoid Function (Ïƒ)**  \n",
        "\n",
        "$ Sigmoid = \\frac{1}{1 + \\exp^{-(wX + b)}}$\n",
        "  \n",
        "$ CrossEntropy = -\\overbrace{t}^{Label} * \\ln({\\overbrace{y}^{Sigmoid}}) - (1-t)*\\ln({1-y})$  \n",
        "\n",
        "$ Loss = \\frac{1}{N}\\sum_{n=1}^{N}Cross Entropy $\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For\n",
        "\n",
        "Write your explanation here. Use Latex to write mathematical expressions. [Here is a brief tutorial on latex for notebooks.](https://www.math.ubc.ca/~pwalls/math-python/jupyter/latex/)\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "    \n",
        "def cross_entropy(t, y):\n",
        "  return -t * np.log(y + np.finfo(float).eps) - (1 - t) * np.log(1 - y + np.finfo(float).eps)\n",
        "\n",
        "def cost(y, t):\n",
        "  return np.mean(cross_entropy(t, y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhQXAKd4zGrE"
      },
      "source": [
        "### Part (c) -- 7%\n",
        "\n",
        "We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n",
        "finite difference rule tells us that for small $h$, we should have\n",
        "\n",
        "$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n",
        "by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpRTD-fozGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb98f4aa-13a3-4691-deb5-83d9d9ff575c"
      },
      "source": [
        "# Your code goes here\n",
        "\n",
        "h = 1e-09\n",
        "\n",
        "t = np.zeros(2,)\n",
        "X = np.ones([2, 90])\n",
        "w = np.zeros(90)\n",
        "b = 1\n",
        "y = pred(w, b, X)\n",
        "y_b_plus = pred(w, b + h, X)\n",
        "\n",
        "cost_y = cost(y, t)\n",
        "cost_y_b_plus = cost(y_b_plus, t)\n",
        "\n",
        "r1 = (cost_y_b_plus - cost_y) / h\n",
        "r2 = derivative_cost(X, y, t)[1]\n",
        "print(\"The analytical results is: \", r1)\n",
        "print(\"The algorithm results is: \", r2)\n",
        "print(\"Gradient difference for w1 (analytical-algorithm): \", r1-r2)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is:  0.7310585470321485\n",
            "The algorithm results is:  0.7310585786300049\n",
            "Gradient difference for w1 (analytical-algorithm):  -3.159785644246682e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTiplTPhzGrF"
      },
      "source": [
        "### Part (d) -- 7%\n",
        "\n",
        "Show that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVTsHgnPzGrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ce6c7f-ab7c-49ce-b0d5-b35d68636286"
      },
      "source": [
        "# Your code goes here. You might find this below code helpful: but it's\n",
        "# up to you to figure out how/why, and how to modify the code\n",
        "\n",
        "h = 1e-9\n",
        "t = np.zeros(2,)\n",
        "X1 = np.random.randn(2, 90)\n",
        "w1 = np.random.randn(90,)\n",
        "b = 0\n",
        "\n",
        "y1 = pred(w1, b, X1)\n",
        "y1_w1_plus = pred(w1 + h, b, X1)\n",
        "\n",
        "cost_y1 = cost(y1, t)\n",
        "cost_y1_w1_plus = cost(y1_w1_plus, t)\n",
        "\n",
        "r1 = (cost_y1_w1_plus - cost_y1) / h\n",
        "r2 = np.sum(derivative_cost(X1, y1, t)[0])\n",
        "print(\"The analytical results is: \", r1)\n",
        "print(\"The algorithm results is: \", r2)\n",
        "print(\"Gradient difference for w1 (analytical-algorithm): \", r1-r2)\n",
        "print(\"\\n\", '='*200, \"\\n\")\n",
        "\n",
        "# #We can also see for each w:\n",
        "# w2 = np.random.randn(90,) \n",
        "# cost_y2_w2 = np.zeros([90,])\n",
        "# cost_y2_w2_plus = np.zeros([90,])\n",
        "# h = 1e-9\n",
        "# for i in range(0, len(w1)):\n",
        "#   w2_zeros = np.zeros([90,])\n",
        "#   w2_zeros_plus = np.zeros([90,])\n",
        "#   X_zeros = np.zeros(np.shape(X1))\n",
        "#   X_zeros[:, i] = X1[:, i]\n",
        "#   w2_zeros[i] = w1[i]\n",
        "#   y2 = pred(w2_zeros, b, X_zeros)\n",
        "#   w2_zeros_plus[i] = w1[i] + h\n",
        "#   y2_w2_plus = pred(w2_zeros_plus, b, X_zeros)\n",
        "#   cost_y2_w2[i] = cost(y2, t)\n",
        "#   cost_y2_w2_plus[i] = cost(y2_w2_plus, t)\n",
        "  \n",
        "\n",
        "# y3 = pred(w1, b, X1)\n",
        "\n",
        "# r3 = (cost_y2_w2_plus - cost_y2_w2) / h\n",
        "# r4 = derivative_cost(X1, y3, t)[0]\n",
        "# print(\"r4 lengtth\", len(r4))\n",
        "# print(\"The analytical results is:\", r3)\n",
        "# print(\"The algorithm results is: \", r4)\n",
        "# print(\"Gradient difference for w2 (analytical-algorithm): \", r3-r4)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analytical results is:  -5.156747424905905e-05\n",
            "The algorithm results is:  -5.1617673715317804e-05\n",
            "Gradient difference for w1 (analytical-algorithm):  5.019946625875113e-08\n",
            "\n",
            " ======================================================================================================================================================================================================== \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgBTPF_2zGrG"
      },
      "source": [
        "### Part (e) -- 7%\n",
        "\n",
        "Now that you have a gradient function that works, we can actually run gradient descent. \n",
        "Complete the following code that will run stochastic: gradient descent training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4DEuuPzGrG"
      },
      "source": [
        "def run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu=0.1, batch_size=100, max_iters=100):\n",
        "  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n",
        "  We use:\n",
        "    - train_norm_xs and train_ts as the training set\n",
        "    - val_norm_xs and val_ts as the test set\n",
        "    - mu as the learning rate\n",
        "    - (w0, b0) as the initial values of (w, b)\n",
        "\n",
        "  Precondition: np.shape(w0) == (90,)\n",
        "                type(b0) == float\n",
        " \n",
        "  Postcondition: np.shape(w) == (90,)\n",
        "                 type(b) == float\n",
        "  \"\"\"\n",
        "  w = w0\n",
        "  b = b0\n",
        "  iter = 0\n",
        "  history = {}\n",
        "  val_cost_history = []\n",
        "  val_acc_history = []\n",
        "\n",
        "  while iter < max_iters:\n",
        "    # shuffle the training set (there is code above for how to do this)\n",
        "    reindex = np.random.permutation(len(train_norm_xs))\n",
        "    train_norm_xs = train_norm_xs[reindex]\n",
        "    train_ts = train_ts[reindex]\n",
        "\n",
        "    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n",
        "      # minibatch that we are working with:\n",
        "      X = train_norm_xs[i:(i + batch_size)]\n",
        "      t = train_ts[i:(i + batch_size), 0]\n",
        "\n",
        "      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n",
        "      # the \"last\" minibatch\n",
        "      if np.shape(X)[0] != batch_size:\n",
        "        continue\n",
        "\n",
        "      # compute the prediction\n",
        "      prediction = pred(w, b, X)\n",
        "\n",
        "      # update w and b\n",
        "      dLdw, dLdb = derivative_cost(X, t, prediction)\n",
        "      w -= dLdw*mu\n",
        "      b -= dLdb*mu\n",
        "\n",
        "      # increment the iteration count\n",
        "    iter += 1\n",
        "      # compute and print the *validation* loss and accuracy\n",
        "    if (iter % 10 == 0):\n",
        "      val_cost = 0\n",
        "      val_acc = 0\n",
        "      count = 0\n",
        "      for i in range(0, len(val_norm_xs), batch_size): # iterate over each minibatch\n",
        "        # minibatch that we are working with:\n",
        "        X = val_norm_xs[i:(i + batch_size)]\n",
        "        t = val_ts[i:(i + batch_size), 0]\n",
        "\n",
        "        val_prediction = pred(w, b, X)\n",
        "        val_cost += cost(t, val_prediction)\n",
        "        val_acc += get_accuracy(t, val_prediction)\n",
        "        count += 1\n",
        "\n",
        "      val_cost /= count\n",
        "      val_acc /= count\n",
        "      print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (\n",
        "              iter, val_acc * 100, val_cost))\n",
        "      val_cost_history.append(val_cost)\n",
        "      val_acc_history.append(val_acc)\n",
        "\n",
        "      if iter >= max_iters:\n",
        "        break\n",
        "\n",
        "      # Think what parameters you should return for further use\n",
        "  history[\"val_cost\"] = val_cost_history\n",
        "  history[\"val_acc\"] = val_acc_history\n",
        "  return history, (w, b)\n",
        "\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MqzT0jGzGrH"
      },
      "source": [
        "### Part (f) -- 7%\n",
        "\n",
        "Call `run_gradient_descent` with the weights and biases all initialized to zero.\n",
        "Show that if the learning rate $\\mu$ is too small, then convergence is slow.\n",
        "Also, show that if $\\mu$ is too large, then the optimization algorirthm does not converge. The demonstration should be made using plots showing these effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE32Iqo6zGrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52fa9fd2-c153-44dd-a968-718a4406187d"
      },
      "source": [
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "# Write your code here\n",
        "print(\"Small mu: \")\n",
        "small_mu_history, parameters_small = run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu=1e-4, batch_size=100, max_iters=200)\n",
        "print(\"\\n\\n\", '='*200, \"\\n\\nLarge mu: \")\n",
        "large_mu_history, parameters_large = run_gradient_descent(train_norm_xs, train_ts, val_norm_xs, val_ts, w0, b0, mu=1e+4, batch_size=100, max_iters=200)\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small mu: \n",
            "Iter 10. [Val Acc 1%, Loss 20.938677]\n",
            "Iter 20. [Val Acc 3%, Loss 20.684086]\n",
            "Iter 30. [Val Acc 6%, Loss 20.462558]\n",
            "Iter 40. [Val Acc 8%, Loss 20.331374]\n",
            "Iter 50. [Val Acc 9%, Loss 20.227136]\n",
            "Iter 60. [Val Acc 11%, Loss 20.149816]\n",
            "Iter 70. [Val Acc 12%, Loss 20.089422]\n",
            "Iter 80. [Val Acc 13%, Loss 20.049233]\n",
            "Iter 90. [Val Acc 14%, Loss 20.001324]\n",
            "Iter 100. [Val Acc 14%, Loss 19.967842]\n",
            "Iter 110. [Val Acc 15%, Loss 19.938820]\n",
            "Iter 120. [Val Acc 15%, Loss 19.914847]\n",
            "Iter 130. [Val Acc 16%, Loss 19.885994]\n",
            "Iter 140. [Val Acc 16%, Loss 19.863896]\n",
            "Iter 150. [Val Acc 16%, Loss 19.833641]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 160. [Val Acc 17%, Loss 19.812343]\n",
            "Iter 170. [Val Acc 17%, Loss 19.795782]\n",
            "Iter 180. [Val Acc 17%, Loss 19.778704]\n",
            "Iter 190. [Val Acc 17%, Loss 19.762282]\n",
            "Iter 200. [Val Acc 17%, Loss 19.742553]\n",
            "\n",
            "\n",
            " ======================================================================================================================================================================================================== \n",
            "\n",
            "Large mu: \n",
            "Iter 10. [Val Acc 47%, Loss 19.121879]\n",
            "Iter 20. [Val Acc 47%, Loss 19.122600]\n",
            "Iter 30. [Val Acc 47%, Loss 19.122600]\n",
            "Iter 40. [Val Acc 47%, Loss 19.121879]\n",
            "Iter 50. [Val Acc 47%, Loss 19.121158]\n",
            "Iter 60. [Val Acc 47%, Loss 19.121158]\n",
            "Iter 70. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 80. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 90. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 100. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 110. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 120. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 130. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 140. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 150. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 160. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 170. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 180. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 190. [Val Acc 47%, Loss 19.120437]\n",
            "Iter 200. [Val Acc 47%, Loss 19.120437]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "def plot_acc_loss(acc_history, loss_history, iteration_resolution):\n",
        "  \"\"\"Plot the values of validation accuracy and loss from model train history lists.\n",
        "  We use:\n",
        "    - acc_historys as the accuracy history list\n",
        "    - loss_history as the loss history list\n",
        "    - iteration_resolution as the values iterations resolution\n",
        "\n",
        "  Precondition: type(acc_history) == list(flaot)\n",
        " \n",
        "  Postcondition: type(loss_history) == list(flaot)\n",
        "  \"\"\"\n",
        "  iterations = range(iteration_resolution, len(acc_history)*iteration_resolution + iteration_resolution, 10)\n",
        "  \n",
        "  plt.figure(figsize=[32,6])\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(iterations[:], acc_history[:], \"b-\")\n",
        "  plt.xlabel(\"iteration\", fontsize=14)\n",
        "  plt.ylabel(\"Validation Accuracy\", rotation=90, fontsize=14)\n",
        "  ax1 = plt.gca()\n",
        "  ax1.set(ylim=(0, 1))\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=[32,6])\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(iterations[:], loss_history[:], \"r-\")\n",
        "  plt.xlabel(\"iteration\", fontsize=14)\n",
        "  plt.ylabel(\"Validation Loss\", rotation=90, fontsize=14)\n",
        "  ax2 = plt.gca()\n",
        "  ax2.set(ylim=(0, max(loss_history) +1))\n",
        "  plt.grid()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "3BGW2webqpY9"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acc_loss(small_mu_history[\"val_acc\"], small_mu_history[\"val_cost\"], 10)"
      ],
      "metadata": {
        "id": "4Ng2E7nTvtvw",
        "outputId": "8d297104-a5e4-4dc0-9aea-b3f309883706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2304x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2304x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acc_loss(large_mu_history[\"val_acc\"], large_mu_history[\"val_cost\"], 10)\n"
      ],
      "metadata": {
        "id": "NvXTevDK18rY",
        "outputId": "3c00a986-bfc1-4b4f-cac1-a1af2df9822e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2304x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2304x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S44GmUZ12o-g"
      },
      "source": [
        "**Explain and discuss your results here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ5HlDFAzGrH"
      },
      "source": [
        "### Part (g) -- 7%\n",
        "\n",
        "Find the optimial value of ${\\bf w}$ and $b$ using your code. Explain how you chose\n",
        "the learning rate $\\mu$ and the batch size. Show plots demostrating good and bad behaviours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dFOFSwgzGrI"
      },
      "source": [
        "w0 = np.random.randn(90)\n",
        "b0 = np.random.randn(1)[0]\n",
        "\n",
        "# Write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZt7_932zX2"
      },
      "source": [
        "**Explain and discuss your results here:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrQqSj2zGrI"
      },
      "source": [
        "### Part (h) -- 15%\n",
        "\n",
        "Using the values of `w` and `b` from part (g), compute your training accuracy, validation accuracy,\n",
        "and test accuracy. Are there any differences between those three values? If so, why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKw2mLozGrI"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "train_acc = ...\n",
        "val_acc = ...\n",
        "test_acc = ...\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXZa1u6920M3"
      },
      "source": [
        "**Explain and discuss your results here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4eP2Yh1zGrI"
      },
      "source": [
        "### Part (i) -- 15%\n",
        "\n",
        "Writing a classifier like this is instructive, and helps you understand what happens when\n",
        "we train a model. However, in practice, we rarely write model building and training code\n",
        "from scratch. Instead, we typically use one of the well-tested libraries available in a package.\n",
        "\n",
        "Use `sklearn.linear_model.LogisticRegression` to build a linear classifier, and make predictions about the test set. Start by reading the\n",
        "[API documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
        "\n",
        "Compute the training, validation and test accuracy of this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LCfAa1zGrJ"
      },
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "model = ...\n",
        "\n",
        "train_acc = ...\n",
        "val_acc = ...\n",
        "test_acc = ...\n",
        "\n",
        "print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqucdV923tG"
      },
      "source": [
        "**This parts helps by checking if the code worked.**\n",
        "**Check if you get similar results, if not repair your code**\n"
      ]
    }
  ]
}